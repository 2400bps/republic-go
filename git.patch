diff --git a/.travis.sh b/.travis.sh
index 3f0b0c8c..aa410b69 100755
--- a/.travis.sh
+++ b/.travis.sh
@@ -1,8 +1,13 @@
+# Simulate the environment used by Travis CI so that we can run local tests to
+# find and resolve issues that are consistent with the Travis CI environment.
+# This is helpful because Travis CI often finds issues that our own local tests
+# do not.
+
 # go vet ./...
 # golint -set_exit_status `go list ./... | grep -Ev "(stackint/asm|vendor)"`
 # golint `go list ./... | grep -Ev "(stackint/asm|vendor)"`
-CI=true ginkgo --cover --coverprofile coverprofile.out ./...
-covermerge cmd/darknode/coverprofile.out crypto/coverprofile.out darknode/coverprofile.out dht/coverprofile.out dispatch/coverprofile.out grpc/coverprofile.out http/coverprofile.out http/adapter/coverprofile.out identity/coverprofile.out ingress/coverprofile.out leveldb/coverprofile.out logger/coverprofile.out ome/coverprofile.out order/coverprofile.out orderbook/coverprofile.out shamir/coverprofile.out smpc/coverprofile.out stackint/coverprofile.out stream/coverprofile.out swarm/coverprofile.out > coverprofile.out
+GOMAXPROCS=1 CI=true ginkgo -v --race --cover --coverprofile coverprofile.out ./...
+covermerge crypto/coverprofile.out darknode/coverprofile.out dht/coverprofile.out dispatch/coverprofile.out grpc/coverprofile.out http/coverprofile.out http/adapter/coverprofile.out identity/coverprofile.out ingress/coverprofile.out leveldb/coverprofile.out logger/coverprofile.out ome/coverprofile.out order/coverprofile.out orderbook/coverprofile.out shamir/coverprofile.out smpc/coverprofile.out stackint/coverprofile.out stream/coverprofile.out swarm/coverprofile.out > coverprofile.out
 sed -i '/.pb.go/d' coverprofile.out
 sed -i '/bindings/d' coverprofile.out
 sed -i '/cmd/d' coverprofile.out
\ No newline at end of file
diff --git a/.travis.yml b/.travis.yml
index 968779e2..2fc61316 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -16,8 +16,8 @@ script:
   # - go vet ./...
   # - golint -set_exit_status `go list ./... | grep -Ev "(stackint/asm|vendor)"`
   # - golint `go list ./... | grep -Ev "(stackint/asm|vendor)"`
-  - ginkgo --cover --coverprofile coverprofile.out ./...
-  - covermerge cmd/darknode/coverprofile.out crypto/coverprofile.out darknode/coverprofile.out dht/coverprofile.out dispatch/coverprofile.out grpc/coverprofile.out http/coverprofile.out http/adapter/coverprofile.out identity/coverprofile.out ingress/coverprofile.out leveldb/coverprofile.out logger/coverprofile.out ome/coverprofile.out order/coverprofile.out orderbook/coverprofile.out shamir/coverprofile.out smpc/coverprofile.out stackint/coverprofile.out stream/coverprofile.out swarm/coverprofile.out > coverprofile.out
+  - ginkgo --race --cover --coverprofile coverprofile.out ./...
+  - covermerge crypto/coverprofile.out darknode/coverprofile.out dht/coverprofile.out dispatch/coverprofile.out grpc/coverprofile.out http/coverprofile.out http/adapter/coverprofile.out identity/coverprofile.out ingress/coverprofile.out leveldb/coverprofile.out logger/coverprofile.out ome/coverprofile.out order/coverprofile.out orderbook/coverprofile.out shamir/coverprofile.out smpc/coverprofile.out stackint/coverprofile.out stream/coverprofile.out swarm/coverprofile.out > coverprofile.out
   - sed -i '/.pb.go/d' coverprofile.out
   - sed -i '/bindings/d' coverprofile.out
   - sed -i '/cmd/d' coverprofile.out
diff --git a/cmd/darknode/darknode.go b/cmd/darknode/darknode.go
index 9a07b30f..baf02abf 100644
--- a/cmd/darknode/darknode.go
+++ b/cmd/darknode/darknode.go
@@ -142,9 +142,8 @@ func main() {
 			for {
 				if err := darknode.Sync(); err != nil {
 					log.Printf("cannot sync darknode: %v", err)
-					continue
 				}
-				time.Sleep(time.Second * 2)
+				time.Sleep(time.Second * 14)
 			}
 		})
 	}()
diff --git a/cmd/darknode/darknode_test.go b/cmd/darknode/darknode_test.go
index cc9f4fd5..84fc50cd 100644
--- a/cmd/darknode/darknode_test.go
+++ b/cmd/darknode/darknode_test.go
@@ -72,6 +72,8 @@ var _ = test.SkipCIDescribe("Darknode integration", func() {
 
 	test.SkipCIAfterSuite(func() {
 		defer mu.Unlock()
+		defer os.RemoveAll("./db.out/")
+		defer ganache.Stop()
 
 		for _, server := range servers {
 			server.Stop()
@@ -87,7 +89,6 @@ var _ = test.SkipCIDescribe("Darknode integration", func() {
 			Expect(err).ShouldNot(HaveOccurred())
 		}
 
-		os.RemoveAll("./db.out/")
 	})
 
 	Context("when bootstrapping into a network", func() {
diff --git a/cmd/ingress/config.kovan.json b/cmd/ingress/config.kovan.json
index 13a7071d..20708099 100644
--- a/cmd/ingress/config.kovan.json
+++ b/cmd/ingress/config.kovan.json
@@ -1,8 +1,8 @@
 {
     "bootstrapMultiAddresses": [
-        "/ip4/13.126.252.99/tcp/18514/republic/8MGbQzE9Umw4enrwgpNGdZUqPcbz9r",
-        "/ip4/54.83.142.110/tcp/18514/republic/8MJpR3iD7vLq4GRkiyCf36uxamYCic",
-        "/ip4/35.159.11.84/tcp/18514/republic/8MGzFA6kTmSYSU7BjZGYK1jTZmmNJ7"
+        "/ip4/35.183.0.47/tcp/18514/republic/8MG8CjSSXNnojNCPQRusLZ8rUxVKaa",
+        "/ip4/35.172.199.20/tcp/18514/republic/8MJQChPBvNDKmYLCHk1PWpgd1RFjgR",
+        "/ip4/34.242.249.47/tcp/18514/republic/8MKG4qaPG1WpUFivwXb5RyH2o49f1h"
     ],
     "ethereum": {
         "network": "kovan",
diff --git a/cmd/ingresscli/config.json b/cmd/ingresscli/config.kovan.json
similarity index 100%
rename from cmd/ingresscli/config.json
rename to cmd/ingresscli/config.kovan.json
diff --git a/cmd/ingresscli/ingresscli.go b/cmd/ingresscli/ingresscli.go
index 29493898..0b60c326 100644
--- a/cmd/ingresscli/ingresscli.go
+++ b/cmd/ingresscli/ingresscli.go
@@ -103,7 +103,6 @@ func main() {
 			marshaledOrdFragment.OrderType = encryptedFragment.OrderType
 			marshaledOrdFragment.OrderExpiry = encryptedFragment.OrderExpiry.Unix()
 			marshaledOrdFragment.Tokens = base64.StdEncoding.EncodeToString(encryptedFragment.Tokens)
-			log.Printf("TOKENS: %v", marshaledOrdFragment.Tokens)
 			marshaledOrdFragment.Price = []string{
 				base64.StdEncoding.EncodeToString(encryptedFragment.Price.Co),
 				base64.StdEncoding.EncodeToString(encryptedFragment.Price.Exp),
diff --git a/cmd/sendOrder/config.json b/cmd/sendOrder/config.json
index 30ce7c8f..801d4bc4 100644
--- a/cmd/sendOrder/config.json
+++ b/cmd/sendOrder/config.json
@@ -1,7 +1,7 @@
 {
-        "network": "kovan",
-        "uri": "https://kovan.infura.io",
-        "republicTokenAddress": "0x596F8c39aEc9fb72D0F591DEe4408516f4C9DdA4",
-        "darknodeRegistryAddress": "0x7b0e5c2945020996408fdf73ef1846d7c0dcac78",
-        "renLedgerAddress": "0x9ac38a5f17aae6d473b0f87bd6e42e8958043c70"
+        "network": "ropsten",
+        "uri": "https://ropsten.infura.io",
+        "republicTokenAddress": "0x65d54EDa5f032F2275Caa557E50c029cFbCCBB54",
+        "darknodeRegistryAddress": "0x69eb8d26157b9e12f959ea9f189A5D75991b59e3",
+        "renLedgerAddress": "0x6235E09103bC7f205837237e4eAD855bC196E4D3"
 }
\ No newline at end of file
diff --git a/cmd/sendOrder/sendOrder.go b/cmd/sendOrder/sendOrder.go
index 63afb10b..e22b2bd1 100644
--- a/cmd/sendOrder/sendOrder.go
+++ b/cmd/sendOrder/sendOrder.go
@@ -96,7 +96,6 @@ func main() {
 				}
 
 				encryptedFragment, err := ordFragment.Encrypt(pubKey)
-				log.Println(encryptedFragment)
 				marshaledOrdFragment.ID = base64.StdEncoding.EncodeToString(encryptedFragment.ID[:])
 				marshaledOrdFragment.OrderID = base64.StdEncoding.EncodeToString(encryptedFragment.OrderID[:])
 				marshaledOrdFragment.OrderParity = encryptedFragment.OrderParity
diff --git a/crypto/ecdsa_test.go b/crypto/ecdsa_test.go
index 68d6a078..add0fa28 100644
--- a/crypto/ecdsa_test.go
+++ b/crypto/ecdsa_test.go
@@ -2,6 +2,9 @@ package crypto_test
 
 import (
 	"crypto/rand"
+	"runtime"
+
+	"github.com/republicprotocol/republic-go/dispatch"
 
 	. "github.com/onsi/ginkgo"
 	. "github.com/onsi/gomega"
@@ -43,7 +46,7 @@ var _ = Describe("Ecdsa keys", func() {
 		})
 
 		It("should be able to verify a signature", func() {
-			for i := 0; i < 1000; i++ {
+			dispatch.CoForAll(runtime.NumCPU(), func(i int) {
 				key, err := RandomEcdsaKey()
 				Expect(err).ShouldNot(HaveOccurred())
 
@@ -53,11 +56,11 @@ var _ = Describe("Ecdsa keys", func() {
 
 				err = key.Verify(data, signature)
 				Expect(err).ShouldNot(HaveOccurred())
-			}
+			})
 		})
 
 		It("should be able to return an error when verifying random data", func() {
-			for i := 0; i < 1000; i++ {
+			dispatch.CoForAll(runtime.NumCPU(), func(i int) {
 				key, err := RandomEcdsaKey()
 				Expect(err).ShouldNot(HaveOccurred())
 
@@ -69,7 +72,45 @@ var _ = Describe("Ecdsa keys", func() {
 
 				err = key.Verify(random, sigRandom)
 				Expect(err).Should(HaveOccurred())
-			}
+			})
+		})
+
+		It("should be able to return an error when verifying nil data", func() {
+			dispatch.CoForAll(runtime.NumCPU(), func(i int) {
+				key, err := RandomEcdsaKey()
+				Expect(err).ShouldNot(HaveOccurred())
+
+				random := make([]byte, 32)
+				rand.Read(random)
+
+				sigRandom := make([]byte, 65)
+				rand.Read(sigRandom)
+
+				err = key.Verify([]byte{}, sigRandom)
+				Expect(err).Should(Equal(ErrNilData))
+
+				err = key.Verify(nil, sigRandom)
+				Expect(err).Should(Equal(ErrNilData))
+			})
+		})
+
+		It("should be able to return an error when verifying nil signatures", func() {
+			dispatch.CoForAll(runtime.NumCPU(), func(i int) {
+				key, err := RandomEcdsaKey()
+				Expect(err).ShouldNot(HaveOccurred())
+
+				random := make([]byte, 32)
+				rand.Read(random)
+
+				sigRandom := make([]byte, 65)
+				rand.Read(sigRandom)
+
+				err = key.Verify(random, []byte{})
+				Expect(err).Should(Equal(ErrNilSignature))
+
+				err = key.Verify(random, nil)
+				Expect(err).Should(Equal(ErrNilSignature))
+			})
 		})
 
 	})
diff --git a/darknode/crypter_test.go b/darknode/crypter_test.go
index aae9396f..5b959561 100644
--- a/darknode/crypter_test.go
+++ b/darknode/crypter_test.go
@@ -4,9 +4,11 @@ import (
 	"bytes"
 	"crypto/rand"
 	"crypto/rsa"
+	"runtime"
 	"time"
 
 	"github.com/republicprotocol/republic-go/cal"
+	"github.com/republicprotocol/republic-go/dispatch"
 	"github.com/republicprotocol/republic-go/identity"
 
 	. "github.com/onsi/ginkgo"
@@ -51,7 +53,7 @@ var _ = Describe("Crypter", func() {
 	Context("when verifying signatures", func() {
 
 		It("should return an error for unregistered addresses", func() {
-			for i := 0; i < 10; i++ {
+			dispatch.CoForAll(runtime.NumCPU(), func(i int) {
 				keystore, err := crypto.RandomKeystore()
 				Expect(err).ShouldNot(HaveOccurred())
 
@@ -59,7 +61,7 @@ var _ = Describe("Crypter", func() {
 				Expect(err).ShouldNot(HaveOccurred())
 				err = crypter.Verify(crypto.Keccak256(message), signature)
 				Expect(err).Should(HaveOccurred())
-			}
+			})
 		})
 
 		It("should not return an error for registered addresses", func() {
@@ -93,13 +95,13 @@ var _ = Describe("Crypter", func() {
 		})
 
 		It("should not encrypt messages for unregistered addresses", func() {
-			for i := 0; i < 10; i++ {
+			dispatch.CoForAll(runtime.NumCPU(), func(i int) {
 				keystore, err := crypto.RandomKeystore()
 				Expect(err).ShouldNot(HaveOccurred())
 
 				_, err = crypter.Encrypt(keystore.Address(), message[:])
 				Expect(err).Should(HaveOccurred())
-			}
+			})
 		})
 
 	})
diff --git a/dispatch/broadcast_test.go b/dispatch/broadcast_test.go
index 9961cbcb..011fc375 100644
--- a/dispatch/broadcast_test.go
+++ b/dispatch/broadcast_test.go
@@ -88,22 +88,80 @@ var _ = Describe("Broadcaster", func() {
 			close(signal)
 		}, 10 /* 10 second timeout */)
 
-		It("should not block existing listeners after shutting down", func() {
-
-		})
-
 		It("should not block new listeners after shutting down", func() {
+			broadcaster := NewBroadcaster()
+			broadcaster.Close()
 
+			done := make(chan struct{})
+			lis := broadcaster.Listen(done)
+			select {
+			case _, ok := <-lis:
+				Expect(ok).Should(BeFalse())
+			default:
+			}
 		})
 
 		It("should not block when shutting down under heavy usage", func() {
+			broadcaster := NewBroadcaster()
+
+			var wg sync.WaitGroup
+			wg.Add(2)
+
+			go func() {
+				defer GinkgoRecover()
+				defer wg.Done()
+
+				CoForAll(int(MaxListeners), func(i int) {
+					done := make(chan struct{})
+					lis := broadcaster.Listen(done)
+					close(done)
+					<-lis
+				})
+			}()
+			time.Sleep(time.Second)
+
+			go func() {
+				defer GinkgoRecover()
+				defer wg.Done()
+				defer broadcaster.Close()
+
+				CoForAll(int(MaxListeners), func(i int) {
+					done := make(chan struct{})
+					ch := make(chan interface{})
+					defer close(ch)
+					go broadcaster.Broadcast(done, ch)
+					ch <- struct{}{}
+				})
+			}()
 
+			wg.Wait()
 		})
 
 	})
 
 	Context("when broadcasting", func() {
 
+		It("should restrict the maximum number of listeners", func() {
+			broadcaster := NewBroadcaster()
+			listeners := make([]<-chan interface{}, 2*MaxListeners)
+			closed := int32(0)
+			CoForAll(int(2*MaxListeners), func(i int) {
+				done := make(chan struct{})
+				lis := broadcaster.Listen(done)
+				listeners[i] = lis
+			})
+			CoForAll(int(2*MaxListeners), func(i int) {
+				select {
+				case _, ok := <-listeners[i]:
+					if !ok {
+						atomic.AddInt32(&closed, 1)
+					}
+				default:
+				}
+			})
+			Expect(closed).Should(Equal(MaxListeners))
+		})
+
 		It("should send message from one broadcast to many listeners", func(done Done) {
 			defer close(done)
 			broadcaster := NewBroadcaster()
@@ -122,9 +180,9 @@ var _ = Describe("Broadcaster", func() {
 						atomic.AddInt64(&n, 1)
 					}
 				})
-				Expect(n).Should(Equal(int64(MaxListeners * 1000)))
+				Expect(n).Should(Equal(int64(MaxListeners)))
 			}()
-			time.Sleep(2 * time.Second)
+			time.Sleep(time.Second)
 
 			go func() {
 				defer GinkgoRecover()
@@ -135,18 +193,12 @@ var _ = Describe("Broadcaster", func() {
 				ch := make(chan interface{})
 				defer close(ch)
 				go broadcaster.Broadcast(done, ch)
-				for i := 0; i < 1000; i++ {
-					ch <- i
-				}
-				time.Sleep(2 * time.Second)
+				ch <- struct{}{}
+				time.Sleep(time.Second)
 			}()
 
 			wg.Wait()
-		}, 10 /* 10 second timeout */)
-
-		It("should send messages from many broadcasts to one listener", func() {
-
-		})
+		}, 30 /* 30 second timeout */)
 
 		It("should send messages from many broadcasts to many listeners", func(done Done) {
 			defer close(done)
@@ -166,29 +218,27 @@ var _ = Describe("Broadcaster", func() {
 						atomic.AddInt64(&n, 1)
 					}
 				})
-				Expect(n).Should(Equal(int64(MaxListeners * 100 * 1000)))
+				Expect(n).Should(Equal(int64(MaxListeners * MaxListeners)))
 			}()
-			time.Sleep(2 * time.Second)
+			time.Sleep(time.Second)
 
 			go func() {
 				defer GinkgoRecover()
 				defer wg.Done()
 				defer broadcaster.Close()
 
-				CoForAll(int(100), func(i int) {
+				CoForAll(int(MaxListeners), func(i int) {
 					done := make(chan struct{})
 					ch := make(chan interface{})
 					defer close(ch)
 					go broadcaster.Broadcast(done, ch)
-					for j := 0; j < 1000; j++ {
-						ch <- j
-					}
+					ch <- struct{}{}
 				})
-				time.Sleep(10 * time.Second)
+				time.Sleep(time.Second * 3)
 			}()
 
 			wg.Wait()
-		}, 120 /* 30 second timeout */)
+		}, 60 /* 1 minute timeout */)
 
 	})
 
diff --git a/dispatch/dispatch.go b/dispatch/dispatch.go
index 0de7034e..2ac8529a 100644
--- a/dispatch/dispatch.go
+++ b/dispatch/dispatch.go
@@ -1,154 +1 @@
 package dispatch
-
-import (
-	"fmt"
-	"reflect"
-	"sync"
-)
-
-// Dispatch functions onto goroutine in the background. Returns a channel that
-// is closed when all goroutines have terminated.
-func Dispatch(fs ...func()) <-chan struct{} {
-	done := make(chan struct{})
-	go func() {
-		defer close(done)
-
-		var wg sync.WaitGroup
-		for _, f := range fs {
-			wg.Add(1)
-			go func(f func()) {
-				defer wg.Done()
-				f()
-			}(f)
-		}
-		wg.Wait()
-	}()
-
-	return done
-}
-
-// Wait waits for multiple signal channels to end
-func Wait(chs ...chan struct{}) {
-	for _, ch := range chs {
-		for range ch {
-			// Pass
-		}
-	}
-}
-
-// Close closes multiple channels
-func Close(chs ...interface{}) {
-	for _, ch := range chs {
-		if reflect.TypeOf(ch).Kind() == reflect.Chan {
-			reflect.ValueOf(ch).Close()
-		}
-	}
-}
-
-// Split splits a channel into multiple channel
-// The input and output channels should be of the same type
-func Split(chIn interface{}, chsOut ...interface{}) {
-	if reflect.TypeOf(chIn).Kind() != reflect.Chan {
-		panic(fmt.Sprintf("cannot split from value of type %T", chIn))
-	}
-	for {
-		msg, ok := reflect.ValueOf(chIn).Recv()
-		if !ok {
-			return
-		}
-		for _, chOut := range chsOut {
-			Send(chOut, msg)
-		}
-	}
-}
-
-// Send sends a msg to a channel or an array of channels
-func Send(chOut interface{}, msgValue reflect.Value) {
-	switch reflect.TypeOf(chOut).Kind() {
-	case reflect.Array, reflect.Slice:
-		for i := 0; i < reflect.ValueOf(chOut).Len(); i++ {
-			if reflect.ValueOf(chOut).Index(i).Kind() != reflect.Chan {
-				panic(fmt.Sprintf("cannot send to type %T", chOut))
-			}
-			reflect.ValueOf(chOut).Index(i).Send(msgValue)
-		}
-	case reflect.Chan:
-		reflect.ValueOf(chOut).Send(msgValue)
-	default:
-		panic(fmt.Sprintf("cannot send to type %T", chOut))
-	}
-}
-
-// Merge merges multiple channels of into a channel
-// The input and output channels should be of the same type
-func Merge(chOut interface{}, chsIn ...interface{}) {
-	if reflect.TypeOf(chOut).Kind() != reflect.Chan {
-		panic(fmt.Sprintf("cannot merge to type %T", chOut))
-	}
-
-	var wg sync.WaitGroup
-
-	mergeCh := func(chIn interface{}) {
-		defer wg.Done()
-		for {
-			msg, ok := reflect.ValueOf(chIn).Recv()
-			if !ok {
-				return
-			}
-			reflect.ValueOf(chOut).Send(msg)
-		}
-	}
-
-	for _, chIn := range chsIn {
-		switch reflect.TypeOf(chIn).Kind() {
-		case reflect.Array, reflect.Slice:
-			for i := 0; i < reflect.ValueOf(chIn).Len(); i++ {
-				if reflect.ValueOf(chIn).Index(i).Kind() != reflect.Chan {
-					panic(fmt.Sprintf("cannot merge from value of type %T", chIn))
-				}
-				wg.Add(1)
-				go mergeCh(reflect.ValueOf(chIn).Index(i).Interface())
-			}
-		case reflect.Chan:
-			wg.Add(1)
-			go mergeCh(chIn)
-		default:
-			panic(fmt.Sprintf("cannot merge from value of type %T", chOut))
-		}
-	}
-
-	wg.Wait()
-}
-
-// Pipe all values from a producer channel to a consumer channel until the
-// producer is closed, and empty, or until the done channel is closed.
-// The consumer channel must not be closed until the Pipe function has
-// returned.
-func Pipe(done <-chan struct{}, producer interface{}, consumer interface{}) {
-	// Type guard the interface inputs
-	if reflect.TypeOf(producer).Kind() != reflect.Chan {
-		panic(fmt.Sprintf("cannot pipe from type %T", producer))
-	}
-	if reflect.TypeOf(consumer).Kind() != reflect.Chan {
-		panic(fmt.Sprintf("cannot pipe to type %T", consumer))
-	}
-	for {
-		cases := [2]reflect.SelectCase{
-			reflect.SelectCase{Dir: reflect.SelectRecv, Chan: reflect.ValueOf(done)},
-			reflect.SelectCase{Dir: reflect.SelectRecv, Chan: reflect.ValueOf(producer)},
-		}
-		i, val, ok := reflect.Select(cases[:])
-		if i == 0 || !ok {
-			return
-		}
-
-		cases = [2]reflect.SelectCase{
-			reflect.SelectCase{Dir: reflect.SelectRecv, Chan: reflect.ValueOf(done)},
-			reflect.SelectCase{Dir: reflect.SelectSend, Chan: reflect.ValueOf(consumer), Send: val},
-		}
-		i, val, ok = reflect.Select(cases[:])
-		if i == 0 {
-			return
-		}
-	}
-}
diff --git a/dispatch/dispatch_test.go b/dispatch/dispatch_test.go
index cddac553..c138b583 100644
--- a/dispatch/dispatch_test.go
+++ b/dispatch/dispatch_test.go
@@ -1,327 +1 @@
 package dispatch_test
-
-import (
-	"reflect"
-	"sync"
-	"time"
-
-	. "github.com/onsi/ginkgo"
-	. "github.com/onsi/gomega"
-	. "github.com/republicprotocol/republic-go/dispatch"
-)
-
-var _ = Describe("Dispatch Package", func() {
-
-	Context("Wait", func() {
-
-		It("should wait for a single signal channels", func() {
-			sigCh := make(chan struct{})
-			signal := true
-
-			go func() {
-				signal = false
-				close(sigCh)
-			}()
-
-			Wait(sigCh)
-
-			Ω(signal).Should(BeFalse())
-		})
-
-		It("should wait for multiple signal channels", func() {
-			sigCh1 := make(chan struct{})
-			sigCh2 := make(chan struct{})
-			sigCh3 := make(chan struct{})
-			sigCh4 := make(chan struct{})
-
-			signal1 := true
-			signal2 := true
-			signal3 := true
-			signal4 := true
-
-			go func() {
-				sigCh1 <- struct{}{}
-				signal1 = false
-				close(sigCh1)
-
-				sigCh2 <- struct{}{}
-				signal2 = false
-				close(sigCh2)
-
-				sigCh3 <- struct{}{}
-				signal3 = false
-				close(sigCh3)
-
-				sigCh4 <- struct{}{}
-				signal4 = false
-				close(sigCh4)
-			}()
-
-			Wait(sigCh1, sigCh2, sigCh3, sigCh4)
-
-			Ω(signal1).Should(BeFalse())
-			Ω(signal2).Should(BeFalse())
-			Ω(signal3).Should(BeFalse())
-			Ω(signal4).Should(BeFalse())
-		})
-
-	})
-
-	Context("Close", func() {
-
-		It("should close a single channel", func() {
-			ch := make(chan int)
-			Close(ch)
-			_, ok := <-ch
-			Ω(ok).Should(BeFalse())
-		})
-
-		It("should close a single channel", func() {
-			ch1 := make(chan int)
-			ch2 := make(chan int)
-			ch3 := make(chan int)
-			ch4 := make(chan int)
-
-			Close(ch1, ch2, ch3, ch4)
-
-			_, ok1 := <-ch1
-			_, ok2 := <-ch2
-			_, ok3 := <-ch3
-			_, ok4 := <-ch4
-
-			Ω(ok1).Should(BeFalse())
-			Ω(ok2).Should(BeFalse())
-			Ω(ok3).Should(BeFalse())
-			Ω(ok4).Should(BeFalse())
-		})
-
-	})
-
-	Context("Split", func() {
-
-		It("should split the channel into an array of channels", func() {
-			inCh := make(chan int)
-			outChs := make([]chan int, 100)
-			for i := 0; i < 100; i++ {
-				outChs[i] = make(chan int)
-			}
-
-			go Split(inCh, outChs)
-
-			go func() {
-				defer close(inCh)
-				inCh <- 1729
-			}()
-
-			for _, ch := range outChs {
-				i := <-ch
-				Ω(i).Should(Equal(1729))
-			}
-		})
-
-		It("should split the channel into multiple channels", func() {
-			inCh := make(chan int)
-			outCh1 := make(chan int)
-			outCh2 := make(chan int)
-			outCh3 := make(chan int)
-
-			go Split(inCh, outCh1, outCh2, outCh3)
-
-			go func() {
-				defer close(inCh)
-				inCh <- 1729
-			}()
-
-			o1 := <-outCh1
-			Ω(o1).Should(Equal(1729))
-
-			o2 := <-outCh2
-			Ω(o2).Should(Equal(1729))
-
-			o3 := <-outCh3
-			Ω(o3).Should(Equal(1729))
-
-		})
-
-		It("should panic when the output channel is of a different type", func() {
-			inCh := make(chan int)
-			outCh := make(chan struct{})
-
-			go func() {
-				defer close(inCh)
-				inCh <- 1729
-			}()
-
-			Ω(func() { Split(inCh, outCh) }).Should(Panic())
-		})
-
-		It("should panic for invalid arguments", func() {
-			in := 10
-			out := false
-
-			Ω(func() { Split(in, out) }).Should(Panic())
-		})
-	})
-
-	Context("Merge", func() {
-
-		It("should merge an array of channels into a channel", func() {
-
-			outCh := make(chan int)
-			inChs := make([]chan int, 100)
-
-			for i := 0; i < 100; i++ {
-				inChs[i] = make(chan int)
-				go func(i int) {
-					defer close(inChs[i])
-					inChs[i] <- i
-				}(i)
-			}
-
-			go Merge(outCh, inChs)
-
-			for i := 0; i < 100; i++ {
-				_ = <-outCh
-
-			}
-		})
-
-		It("should merge multiple channels into a channel", func() {
-			outCh := make(chan int)
-			inCh1 := make(chan int)
-			inCh2 := make(chan int)
-			inCh3 := make(chan int)
-
-			go Merge(outCh, inCh1, inCh2, inCh3)
-
-			go func() {
-				defer close(inCh1)
-				defer close(inCh2)
-				defer close(inCh3)
-				inCh1 <- 1
-				inCh2 <- 2
-				inCh3 <- 3
-			}()
-
-			_ = <-outCh
-			_ = <-outCh
-			_ = <-outCh
-
-			close(outCh)
-		})
-
-		It("should panic for invalid arguments", func() {
-			in := 10
-			out := false
-
-			Ω(func() { Merge(in, out) }).Should(Panic())
-		})
-
-		It("should panic for invalid arguments", func() {
-			in := make(chan int)
-			out := [5]int{1, 2, 3, 4, 5}
-
-			Ω(func() { Merge(in, out) }).Should(Panic())
-		})
-
-		It("should panic for invalid arguments", func() {
-			in := make(chan int)
-			out := 10
-
-			Ω(func() { Merge(in, out) }).Should(Panic())
-		})
-
-	})
-
-	Context("Send", func() {
-		It("should send a message to a channel", func() {
-			in := make(chan int)
-			msg := 1
-
-			go Send(in, reflect.ValueOf(msg))
-
-			Ω(<-in).Should(Equal(msg))
-		})
-
-		It("should send a message to an array of channel", func() {
-			in := make([]chan int, 10)
-			msg := 1
-
-			for i := 0; i < 10; i++ {
-				in[i] = make(chan int)
-			}
-
-			go Send(in, reflect.ValueOf(msg))
-
-			for i := 0; i < 10; i++ {
-				Ω(<-in[i]).Should(Equal(msg))
-			}
-		})
-
-		It("should panic if the message and channel have different types", func() {
-			in := make(chan int)
-			msg := 1
-
-			go func() {
-				time.Sleep(time.Second)
-				close(in)
-			}()
-
-			Ω(func() { Send(in, reflect.ValueOf(msg)) }).Should(Panic())
-		})
-
-		It("should panic for invalid arguments", func() {
-			in := 2
-			msg := 1
-
-			Ω(func() { Send(in, reflect.ValueOf(msg)) }).Should(Panic())
-		})
-
-		It("should panic for invalid arguments", func() {
-			in := []int{1, 2, 3, 4, 5}
-			msg := 1
-
-			Ω(func() { Send(in, reflect.ValueOf(msg)) }).Should(Panic())
-		})
-
-	})
-
-	Context("Pipe", func() {
-		It("should be able to pipe from one channel to another", func() {
-			doneCh := make(chan struct{})
-			inCh := make(chan int)
-			outCh := make(chan int)
-
-			var wg sync.WaitGroup
-
-			wg.Add(1)
-			go func() {
-				defer wg.Done()
-				Pipe(doneCh, inCh, outCh)
-			}()
-
-			for i := 0; i < 10; i++ {
-				inCh <- i
-				Ω(<-outCh).Should(Equal(i))
-			}
-
-			close(doneCh)
-			wg.Wait()
-
-			close(inCh)
-			close(outCh)
-		})
-
-		It("should panic for an invalid producer", func() {
-			doneCh := make(chan struct{})
-			outCh := make(chan int)
-			Ω(func() { Pipe(doneCh, 10, outCh) }).Should(Panic())
-		})
-
-		It("should panic for an invalid consumer", func() {
-			doneCh := make(chan struct{})
-			inCh := make(chan int)
-			Ω(func() { Pipe(doneCh, inCh, 10) }).Should(Panic())
-		})
-	})
-})
diff --git a/dispatch/error_test.go b/dispatch/error_test.go
index 600b1903..395a6cf7 100644
--- a/dispatch/error_test.go
+++ b/dispatch/error_test.go
@@ -12,7 +12,7 @@ import (
 
 var _ = Describe("Error channels", func() {
 
-	Context("Merge errors", func() {
+	Context("when merging errors", func() {
 
 		It("should merge multiple error channels", func() {
 			errCh1 := make(chan error, 1)
@@ -30,7 +30,9 @@ var _ = Describe("Error channels", func() {
 			errCh := MergeErrors(errCh1, errCh2, errCh3)
 			time.Sleep(time.Second)
 
-			Close(errCh1, errCh2, errCh3)
+			close(errCh1)
+			close(errCh2)
+			close(errCh3)
 			Ω(len(errCh)).Should(Equal(3))
 		})
 
@@ -52,7 +54,9 @@ var _ = Describe("Error channels", func() {
 			time.Sleep(time.Second)
 			Ω(len(errCh)).Should(Equal(3))
 
-			Close(errCh1, errCh2, errCh3)
+			close(errCh1)
+			close(errCh2)
+			close(errCh3)
 
 			for err := range errCh {
 				Ω(err == err1 || err == err2 || err == err3).Should(BeTrue())
@@ -62,7 +66,7 @@ var _ = Describe("Error channels", func() {
 
 	})
 
-	Context("Filter errors", func() {
+	Context("when filtering errors", func() {
 		It("should filter errors using a predicate", func() {
 			errCh := make(chan error, 3)
 
@@ -89,12 +93,11 @@ var _ = Describe("Error channels", func() {
 			err := <-filteredErrCh
 
 			Ω(err.Error()).Should(Equal("20"))
-
-			Close(errCh)
+			close(errCh)
 		})
 	})
 
-	Context("Consume errors", func() {
+	Context("when consuming errors", func() {
 		It("should be able to process an error", func() {
 			errCh := make(chan error, 3)
 			defer close(errCh)
diff --git a/docs/docs/logger.md b/docs/docs/logger.md
new file mode 100644
index 00000000..b006327c
--- /dev/null
+++ b/docs/docs/logger.md
@@ -0,0 +1,81 @@
+# Logger
+
+A Logger is an logging object that generates lines of output to an logger plugin.
+Each plugin will have its own implementation of outputing the logs. Please refer 
+[Documentation](https://godoc.org/github.com/republicprotocol/republic-go/logger) 
+for detail usage.
+
+#### Plugin
+
+Currently we support two kinds of plugin, file plugin which will output the logs 
+to a system file (or stdout) and websocket plugin which pumps logs through
+websocket. 
+
+#### Option
+
+Logger will generated with a set of options, usually loaded from a config file.
+Here's an example of the `StdoutLogger` with a pair of tags.
+
+```json
+"logger" : {
+    "plugins": [
+      {
+        "file": {
+          "path": "stdout"
+        }
+      }
+    ] , 
+    "tags" : {
+      "falconry": "true"
+    },
+    "filterLevel": 2,
+    "filterEvents": [
+      "network",
+      "ethereum",
+    ]
+  },
+```
+
+#### Tags
+
+You can define a set of tags as key-value pairs in the logger so that each message 
+generate by the logger will have all the tags attached. In this case, you can
+easily filter the logs by using the tags.
+
+
+#### Log Levels
+
+We define 6 different logging levels, in order from most critical to least:
+
+1. `LevelError`       something critically wrong occurred
+2. `LevelWarn`        something bad occurred but it was not critical
+3. `LevelInfo`        some helpful information 
+4. `LevelDebugHigh`   important debugging information
+5. `LevelDebug`       general debugging information
+6. `LevelDebugLow`    unimportant debugging information
+
+The logs levels can be filtered using `SetFilterLevel(Level)`, which defines the
+highest possible level message that will be logged. The `FilterLevel` by default is
+set to 2 which will only show `LevelError` and `LevelWarn` messages.
+`SetFilterLevel(0)` will disable all log messages (not recommended), and
+`SetFilterLevel(6)` will show all messages.
+
+
+#### Event Types
+
+Each log message can also be classified into Event Types. The Event Types we define
+are:
+
+- `TypeGeneric` 
+- `TypeEpoch`   
+- `TypeUsage` 
+- `TypeEthereum`
+- `TypeOrderMatch`
+- `TypeOrderReceived`
+- `TypeNetwork`
+- `TypeCompute`
+
+The `SetFilterEvents([]EventType)` acts as a whitelist for specific types of events.
+For example, if you were only interested in viewing network and Ethereum logs, you
+could use `SetFilterEvents([]EventType{TypeNetwork, TypeEthereum})` which would
+hide all other types of events.
diff --git a/docs/docs/logs.md b/docs/docs/logs.md
deleted file mode 100644
index 07ef3fbc..00000000
--- a/docs/docs/logs.md
+++ /dev/null
@@ -1,60 +0,0 @@
-# Logger
-
-A Logger is an logging object that generates lines of output to an logger plugin.
-Each plugin will have its own implementation of outputing the logs. Please refer 
-[Documentation](https://godoc.org/github.com/republicprotocol/republic-go/logger) 
-for detail usage.
-
-#### Plugin
-
-Currently we support two kinds of plugin, file plugin which will output the logs 
-to a system file (or stdout) and websocket plugin which pumps logs through
-websocket. 
-
-#### Option
-
-Logger will generated with a set of options, usually loaded from a config file.
-Here's an example of the `StdoutLogger` with a pair of tags.
-
-```json
-"logger" : {
-    "plugins": [
-      {
-        "file": {
-          "path": "stdout"
-        }
-      }
-    ] , 
-    "tags" : {
-      "falconry": "true"
-    }
-  },
-```
-
-#### Tags
-
-You can define a set of tags as key-value pairs in the logger so that each message 
-generate by the logger will have all the tags attached. In this case, you can easily 
-filter the logs by using the tags.
-
-
-#### Log type and event type
-
-
-Log type shows the type of the log message.
-
-- `info` normal information or message
-- `warn`  warning 
-- `error` something wrong happens
-
-Event type shows the event relating to the message which have below categories.
-
-- `generic` 
-- `epoch`   
-- `usage` 
-- `ethereum`
-- `orderMatch`
-- `orderReceived`
-- `network`
-- `compute`
- 
\ No newline at end of file
diff --git a/grpc/orderbook.go b/grpc/orderbook.go
index 1d2c83e9..0b9b5dc0 100644
--- a/grpc/orderbook.go
+++ b/grpc/orderbook.go
@@ -2,7 +2,6 @@ package grpc
 
 import (
 	"fmt"
-	"log"
 	"time"
 
 	"github.com/republicprotocol/republic-go/identity"
@@ -11,29 +10,6 @@ import (
 	"golang.org/x/net/context"
 )
 
-type OrderbookService struct {
-	server orderbook.Server
-}
-
-// NewOrderbookService returns a gRPC service that unmarshals
-// EncryptedOrderFragments and delegates control to an orderbook.Service.
-func NewOrderbookService(server orderbook.Server) OrderbookService {
-	return OrderbookService{
-		server: server,
-	}
-}
-
-// Register the OrderbookService to a Server.
-func (service *OrderbookService) Register(server *Server) {
-	RegisterOrderbookServiceServer(server.Server, service)
-}
-
-// OpenOrder implements the gRPC service for receiving EncryptedOrderFragments.
-func (service *OrderbookService) OpenOrder(ctx context.Context, request *OpenOrderRequest) (*OpenOrderResponse, error) {
-	log.Printf("opening order using order fragment...")
-	return &OpenOrderResponse{}, service.server.OpenOrder(ctx, unmarshalEncryptedOrderFragment(request.OrderFragment))
-}
-
 type orderbookClient struct {
 	connPool *ConnPool
 }
@@ -62,6 +38,28 @@ func (client *orderbookClient) OpenOrder(ctx context.Context, multiAddr identity
 	return err
 }
 
+type OrderbookService struct {
+	server orderbook.Server
+}
+
+// NewOrderbookService returns a gRPC service that unmarshals
+// EncryptedOrderFragments and delegates control to an orderbook.Service.
+func NewOrderbookService(server orderbook.Server) OrderbookService {
+	return OrderbookService{
+		server: server,
+	}
+}
+
+// Register the OrderbookService to a Server.
+func (service *OrderbookService) Register(server *Server) {
+	RegisterOrderbookServiceServer(server.Server, service)
+}
+
+// OpenOrder implements the gRPC service for receiving EncryptedOrderFragments.
+func (service *OrderbookService) OpenOrder(ctx context.Context, request *OpenOrderRequest) (*OpenOrderResponse, error) {
+	return &OpenOrderResponse{}, service.server.OpenOrder(ctx, unmarshalEncryptedOrderFragment(request.OrderFragment))
+}
+
 func marshalEncryptedOrderFragment(orderFragmentIn order.EncryptedFragment) *EncryptedOrderFragment {
 	return &EncryptedOrderFragment{
 		OrderId:     orderFragmentIn.OrderID[:],
diff --git a/grpc/orderbook_test.go b/grpc/orderbook_test.go
index b4865ab5..d828cef0 100644
--- a/grpc/orderbook_test.go
+++ b/grpc/orderbook_test.go
@@ -1 +1,85 @@
 package grpc_test
+
+import (
+	"context"
+	"fmt"
+	"sync/atomic"
+	"time"
+
+	"github.com/republicprotocol/republic-go/crypto"
+	"github.com/republicprotocol/republic-go/order"
+
+	. "github.com/onsi/ginkgo"
+	. "github.com/onsi/gomega"
+	. "github.com/republicprotocol/republic-go/grpc"
+	"github.com/republicprotocol/republic-go/orderbook"
+
+	"github.com/republicprotocol/republic-go/identity"
+)
+
+var _ = Describe("Orderbook", func() {
+
+	var serverMock *mockOrderbookServer
+	var server *Server
+	var service OrderbookService
+	var serviceMultiAddr identity.MultiAddress
+	var client orderbook.Client
+
+	BeforeEach(func() {
+		var err error
+
+		connPool := NewConnPool(128)
+		client = NewOrderbookClient(&connPool)
+
+		serverMock = &mockOrderbookServer{}
+		server = NewServer()
+		service = NewOrderbookService(serverMock)
+		service.Register(server)
+
+		serviceEcdsaKey, err := crypto.RandomEcdsaKey()
+		Expect(err).ShouldNot(HaveOccurred())
+
+		serviceMultiAddr, err = identity.NewMultiAddressFromString(fmt.Sprintf("/ip4/0.0.0.0/tcp/18514/republic/%v", serviceEcdsaKey.Address()))
+		Expect(err).ShouldNot(HaveOccurred())
+
+		go server.Start("0.0.0.0:18514")
+	})
+
+	AfterEach(func() {
+		server.Stop()
+	})
+
+	Context("when opening order fragments", func() {
+
+		It("should accept order fragments", func() {
+			orderFragment, err := createEncryptedFragment()
+			Expect(err).ShouldNot(HaveOccurred())
+			err = client.OpenOrder(context.Background(), serviceMultiAddr, orderFragment)
+			Expect(err).ShouldNot(HaveOccurred())
+		})
+
+	})
+
+})
+
+type mockOrderbookServer struct {
+	n int64
+}
+
+func (server *mockOrderbookServer) OpenOrder(ctx context.Context, orderFragment order.EncryptedFragment) error {
+	atomic.AddInt64(&server.n, 1)
+	return nil
+}
+
+func createEncryptedFragment() (order.EncryptedFragment, error) {
+	ord := order.NewOrder(order.TypeMidpoint, order.ParityBuy, time.Now().Add(time.Hour), order.TokensBTCETH, order.NewCoExp(1, 1), order.NewCoExp(1, 1), order.NewCoExp(1, 1), 1)
+	ordFragments, err := ord.Split(6, 4)
+	if err != nil {
+		return order.EncryptedFragment{}, err
+	}
+	rsaKey, err := crypto.RandomRsaKey()
+	if err != nil {
+		return order.EncryptedFragment{}, err
+	}
+	return ordFragments[0].Encrypt(rsaKey.PublicKey)
+}
diff --git a/grpc/status_test.go b/grpc/status_test.go
index b4865ab5..4d5f699e 100644
--- a/grpc/status_test.go
+++ b/grpc/status_test.go
@@ -1 +1,59 @@
 package grpc_test
+
+import (
+	"context"
+	"fmt"
+	"net"
+
+	. "github.com/onsi/ginkgo"
+	. "github.com/onsi/gomega"
+	"github.com/republicprotocol/republic-go/crypto"
+	"github.com/republicprotocol/republic-go/dht"
+	. "github.com/republicprotocol/republic-go/grpc"
+
+	"github.com/republicprotocol/republic-go/identity"
+)
+
+var _ = Describe("Status", func() {
+
+	var serviceMultiAddr identity.MultiAddress
+	var service StatusService
+	var server *Server
+
+	BeforeEach(func() {
+		var err error
+
+		keystore, err := crypto.RandomKeystore()
+		Expect(err).ShouldNot(HaveOccurred())
+
+		serviceMultiAddr, err = identity.NewMultiAddressFromString(fmt.Sprintf("/ip4/0.0.0.0/tcp/18514/republic/%v", keystore.Address()))
+		Expect(err).ShouldNot(HaveOccurred())
+
+		dht := dht.NewDHT(serviceMultiAddr.Address(), 128)
+		service = NewStatusService(&dht)
+		server = NewServer()
+		service.Register(server)
+
+		lis, err := net.Listen("tcp", "0.0.0.0:18514")
+		Expect(err).ShouldNot(HaveOccurred())
+
+		go server.Serve(lis)
+	})
+
+	AfterEach(func() {
+		server.Stop()
+	})
+
+	Context("when getting the status", func() {
+		It("should return the expected status information", func() {
+			conn, err := Dial(context.Background(), serviceMultiAddr)
+			Expect(err).ShouldNot(HaveOccurred())
+
+			client := NewStatusServiceClient(conn.ClientConn)
+			status, err := client.Status(context.Background(), &StatusRequest{})
+			Expect(err).ShouldNot(HaveOccurred())
+
+			Expect(status.Address).Should(Equal(serviceMultiAddr.Address().String()))
+		})
+	})
+})
diff --git a/grpc/stream.go b/grpc/stream.go
index 72c5df58..b2635c31 100644
--- a/grpc/stream.go
+++ b/grpc/stream.go
@@ -3,11 +3,11 @@ package grpc
 import (
 	"errors"
 	"fmt"
-	"log"
 	"sync"
 
 	"github.com/republicprotocol/republic-go/crypto"
 	"github.com/republicprotocol/republic-go/identity"
+	"github.com/republicprotocol/republic-go/logger"
 	"github.com/republicprotocol/republic-go/stream"
 	"golang.org/x/net/context"
 	"google.golang.org/grpc"
@@ -137,16 +137,12 @@ func NewStreamClient(signer crypto.Signer, addr identity.Address) stream.Client
 }
 
 func (client *streamClient) Connect(ctx context.Context, multiAddr identity.MultiAddress) (stream.Stream, error) {
-	log.Println("dialing", multiAddr)
-
 	// Establish a connection to the identity.MultiAddress
 	conn, err := Dial(ctx, multiAddr)
 	if err != nil {
 		return nil, fmt.Errorf("cannot dial %v: %v", multiAddr, err)
 	}
 
-	log.Println("about to connect..")
-
 	// Open a bidirectional stream and continue to backoff the connection
 	// until the context.Context is canceled
 	var stream StreamService_ConnectClient
@@ -157,8 +153,6 @@ func (client *streamClient) Connect(ctx context.Context, multiAddr identity.Mult
 		return nil, fmt.Errorf("cannot open stream: %v", err)
 	}
 
-	log.Println("connected!")
-
 	// Sign an authentication message so that the StreamService can verify that
 	// the identity.Address of the StreamClient
 	data := []byte(fmt.Sprintf("Republic Protocol: connect: from %v to %v", client.addr, multiAddr.Address()))
@@ -179,8 +173,6 @@ func (client *streamClient) Connect(ctx context.Context, multiAddr identity.Mult
 		return nil, fmt.Errorf("cannot send stream address: %v", err)
 	}
 
-	log.Println("authenticated!")
-
 	// Return a grpc.ClientStream that implements the stream.Stream interface
 	// and is safe for concurrent use and will clean the grpc.ClientConn when
 	// it is no longer needed
@@ -249,8 +241,10 @@ func (service *StreamService) Connect(stream StreamService_ConnectServer) error
 	// done
 	select {
 	case <-stream.Context().Done():
+		logger.Network(logger.LevelDebugLow, "grpc connection closed by client")
 		return stream.Context().Err()
 	case <-s.done:
+		logger.Network(logger.LevelDebugLow, "grpc connection closed by service")
 		return nil
 	}
 }
diff --git a/grpc/stream_test.go b/grpc/stream_test.go
index e569f84c..a5950020 100644
--- a/grpc/stream_test.go
+++ b/grpc/stream_test.go
@@ -9,9 +9,10 @@ import (
 
 	. "github.com/onsi/ginkgo"
 	. "github.com/onsi/gomega"
+	. "github.com/republicprotocol/republic-go/grpc"
+
 	"github.com/republicprotocol/republic-go/crypto"
 	"github.com/republicprotocol/republic-go/dispatch"
-	. "github.com/republicprotocol/republic-go/grpc"
 	"github.com/republicprotocol/republic-go/identity"
 	"github.com/republicprotocol/republic-go/stream"
 )
@@ -28,13 +29,13 @@ var _ = Describe("Streaming", func() {
 	BeforeEach(func() {
 		var err error
 
-		server = NewServer()
-		service, serviceAddr, err = newStreamService()
+		client, clientAddr, err = newStreamClient()
 		Expect(err).ShouldNot(HaveOccurred())
-		service.Register(server)
 
-		client, clientAddr, err = newStreamClient()
+		server = NewServer()
+		service, serviceAddr, err = newStreamService(clientAddr)
 		Expect(err).ShouldNot(HaveOccurred())
+		service.Register(server)
 
 		serviceMultiAddr, err = identity.NewMultiAddressFromString(fmt.Sprintf("/ip4/0.0.0.0/tcp/18514/republic/%v", serviceAddr))
 		Expect(err).ShouldNot(HaveOccurred())
@@ -203,17 +204,17 @@ func newStreamClient() (stream.Client, identity.Address, error) {
 		return nil, identity.Address(""), err
 	}
 	addr := identity.Address(ecdsaKey.Address())
-	client := NewStreamClient(mockSigner{}, addr)
+	client := NewStreamClient(&ecdsaKey, addr)
 	return client, addr, nil
 }
 
-func newStreamService() (*StreamService, identity.Address, error) {
+func newStreamService(clientAddr identity.Address) (*StreamService, identity.Address, error) {
 	ecdsaKey, err := crypto.RandomEcdsaKey()
 	if err != nil {
 		return nil, identity.Address(""), err
 	}
 	addr := identity.Address(ecdsaKey.Address())
-	service := NewStreamService(mockVerifier{}, addr)
+	service := NewStreamService(crypto.NewEcdsaVerifier(clientAddr.String()), addr)
 	return &service, addr, nil
 }
 
diff --git a/grpc/swarm_test.go b/grpc/swarm_test.go
index b4865ab5..b97c2393 100644
--- a/grpc/swarm_test.go
+++ b/grpc/swarm_test.go
@@ -1 +1,120 @@
 package grpc_test
+
+import (
+	"context"
+	"fmt"
+	"time"
+
+	. "github.com/onsi/ginkgo"
+	. "github.com/onsi/gomega"
+	"github.com/republicprotocol/republic-go/dht"
+	. "github.com/republicprotocol/republic-go/grpc"
+	"github.com/republicprotocol/republic-go/swarm"
+
+	"github.com/republicprotocol/republic-go/crypto"
+	"github.com/republicprotocol/republic-go/identity"
+)
+
+var _ = Describe("Swarming", func() {
+
+	var server *Server
+	var service SwarmService
+	var serviceDHT dht.DHT
+	var serviceMultiAddr identity.MultiAddress
+	var serviceClient swarm.Client
+	var client swarm.Client
+
+	BeforeEach(func() {
+		var err error
+
+		serviceClient, err = newSwarmClient()
+		Expect(err).ShouldNot(HaveOccurred())
+
+		serviceDHT = dht.NewDHT(serviceClient.MultiAddress().Address(), 20)
+		service = NewSwarmService(swarm.NewServer(serviceClient, &serviceDHT))
+		serviceMultiAddr = serviceClient.MultiAddress()
+		server = NewServer()
+		service.Register(server)
+
+		client, err = newSwarmClient()
+		Expect(err).ShouldNot(HaveOccurred())
+	})
+
+	AfterEach(func() {
+		server.Stop()
+	})
+
+	Context("when pinging a service", func() {
+
+		It("should return the multiaddress of the service", func(done Done) {
+			defer close(done)
+
+			go func() {
+				defer GinkgoRecover()
+
+				err := server.Start("0.0.0.0:18514")
+				Expect(err).ShouldNot(HaveOccurred())
+			}()
+			time.Sleep(time.Millisecond)
+
+			multiAddr, err := client.Ping(context.Background(), serviceMultiAddr)
+			Expect(err).ShouldNot(HaveOccurred())
+			Expect(multiAddr.String()).Should(Equal(serviceMultiAddr.String()))
+		})
+
+		It("should add the client to the service DHT", func(done Done) {
+			defer close(done)
+
+			go func() {
+				defer GinkgoRecover()
+
+				err := server.Start("0.0.0.0:18514")
+				Expect(err).ShouldNot(HaveOccurred())
+			}()
+			time.Sleep(time.Millisecond)
+
+			_, err := client.Ping(context.Background(), serviceMultiAddr)
+			Expect(err).ShouldNot(HaveOccurred())
+			Expect(serviceDHT.MultiAddresses()).Should(HaveLen(1))
+		})
+
+	})
+
+	Context("when querying a service", func() {
+
+		It("should return the multiaddress of the service close to the query", func(done Done) {
+			defer close(done)
+
+			go func() {
+				defer GinkgoRecover()
+
+				err := server.Start("0.0.0.0:18514")
+				Expect(err).ShouldNot(HaveOccurred())
+			}()
+			time.Sleep(time.Millisecond)
+
+			_, err := client.Ping(context.Background(), serviceMultiAddr)
+			Expect(err).ShouldNot(HaveOccurred())
+
+			multiAddrs, err := client.Query(context.Background(), serviceMultiAddr, client.MultiAddress().Address(), [65]byte{})
+			Expect(err).ShouldNot(HaveOccurred())
+			Expect(multiAddrs).Should(HaveLen(1))
+		})
+
+	})
+})
+
+func newSwarmClient() (swarm.Client, error) {
+	ecdsaKey, err := crypto.RandomEcdsaKey()
+	if err != nil {
+		return nil, err
+	}
+	addr := identity.Address(ecdsaKey.Address())
+	multiAddr, err := identity.NewMultiAddressFromString(fmt.Sprintf("/ip4/0.0.0.0/tcp/18514/republic/%v", addr))
+	if err != nil {
+		return nil, err
+	}
+	connPool := NewConnPool(128)
+	client := NewSwarmClient(multiAddr, &connPool)
+	return client, nil
+}
diff --git a/http/adapter/ingress.go b/http/adapter/ingress.go
index 0090e1af..d9cde1ab 100644
--- a/http/adapter/ingress.go
+++ b/http/adapter/ingress.go
@@ -28,12 +28,12 @@ func NewIngressAdapter(ingress ingress.Ingress) IngressAdapter {
 }
 
 func (adapter *IngressAdapter) OpenOrder(signatureIn string, orderFragmentMappingIn OrderFragmentMapping) error {
-	signature, err := adapter.unmarshalSignature(signatureIn)
+	signature, err := UnmarshalSignature(signatureIn)
 	if err != nil {
 		return err
 	}
 
-	orderID, orderFragmentMapping, err := adapter.unmarshalOrderFragmentMapping(orderFragmentMappingIn)
+	orderID, orderFragmentMapping, err := UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
 	if err != nil {
 		return err
 	}
@@ -46,12 +46,12 @@ func (adapter *IngressAdapter) OpenOrder(signatureIn string, orderFragmentMappin
 }
 
 func (adapter *IngressAdapter) CancelOrder(signatureIn string, orderIDIn string) error {
-	signature, err := adapter.unmarshalSignature(signatureIn)
+	signature, err := UnmarshalSignature(signatureIn)
 	if err != nil {
 		return err
 	}
 
-	orderID, err := adapter.unmarshalOrderID(orderIDIn)
+	orderID, err := UnmarshalOrderID(orderIDIn)
 	if err != nil {
 		return err
 	}
@@ -59,7 +59,11 @@ func (adapter *IngressAdapter) CancelOrder(signatureIn string, orderIDIn string)
 	return adapter.Ingress.CancelOrder(signature, orderID)
 }
 
-func (adapter *IngressAdapter) unmarshalSignature(signatureIn string) ([65]byte, error) {
+func MarshalSignature(signatureIn [65]byte) string {
+	return base64.StdEncoding.EncodeToString(signatureIn[:])
+}
+
+func UnmarshalSignature(signatureIn string) ([65]byte, error) {
 	signature := [65]byte{}
 	signatureBytes, err := base64.StdEncoding.DecodeString(signatureIn)
 	if err != nil {
@@ -72,7 +76,11 @@ func (adapter *IngressAdapter) unmarshalSignature(signatureIn string) ([65]byte,
 	return signature, nil
 }
 
-func (adapter *IngressAdapter) unmarshalOrderID(orderIDIn string) (order.ID, error) {
+func MarshalOrderID(orderIDIn order.ID) string {
+	return base64.StdEncoding.EncodeToString(orderIDIn[:])
+}
+
+func UnmarshalOrderID(orderIDIn string) (order.ID, error) {
 	orderID := order.ID{}
 	orderIDBytes, err := base64.StdEncoding.DecodeString(orderIDIn)
 	if err != nil {
@@ -85,7 +93,11 @@ func (adapter *IngressAdapter) unmarshalOrderID(orderIDIn string) (order.ID, err
 	return orderID, nil
 }
 
-func (adapter *IngressAdapter) unmarshalOrderFragmentID(orderFragmentIDIn string) (order.FragmentID, error) {
+func MarshalOrderFragmentID(orderFragmentIDIn order.FragmentID) string {
+	return base64.StdEncoding.EncodeToString(orderFragmentIDIn[:])
+}
+
+func UnmarshalOrderFragmentID(orderFragmentIDIn string) (order.FragmentID, error) {
 	orderFragmentID := order.FragmentID{}
 	orderFragmentIDBytes, err := base64.StdEncoding.DecodeString(orderFragmentIDIn)
 	if err != nil {
@@ -98,7 +110,14 @@ func (adapter *IngressAdapter) unmarshalOrderFragmentID(orderFragmentIDIn string
 	return orderFragmentID, nil
 }
 
-func (adapter *IngressAdapter) unmarshalEncryptedCoExpShare(valueIn []string) (order.EncryptedCoExpShare, error) {
+func MarshalEncryptedCoExpShare(valueIn order.EncryptedCoExpShare) []string {
+	return []string{
+		base64.StdEncoding.EncodeToString(valueIn.Co),
+		base64.StdEncoding.EncodeToString(valueIn.Exp),
+	}
+}
+
+func UnmarshalEncryptedCoExpShare(valueIn []string) (order.EncryptedCoExpShare, error) {
 	var err error
 	value := order.EncryptedCoExpShare{}
 	if len(valueIn) != 2 {
@@ -115,15 +134,30 @@ func (adapter *IngressAdapter) unmarshalEncryptedCoExpShare(valueIn []string) (o
 	return value, nil
 }
 
-func (adapter *IngressAdapter) unmarshalOrderFragment(orderFragmentIn OrderFragment) (ingress.OrderFragment, error) {
+func MarshalOrderFragment(orderFragmentIn ingress.OrderFragment) OrderFragment {
+	orderFragment := OrderFragment{}
+	orderFragment.Index = orderFragmentIn.Index
+	orderFragment.OrderID = MarshalOrderID(orderFragmentIn.OrderID)
+	orderFragment.OrderType = orderFragmentIn.OrderType
+	orderFragment.OrderParity = orderFragmentIn.OrderParity
+	orderFragment.OrderExpiry = orderFragmentIn.OrderExpiry.Unix()
+	orderFragment.ID = MarshalOrderFragmentID(orderFragmentIn.ID)
+	orderFragment.Tokens = base64.StdEncoding.EncodeToString(orderFragmentIn.Tokens)
+	orderFragment.Price = MarshalEncryptedCoExpShare(orderFragmentIn.Price)
+	orderFragment.Volume = MarshalEncryptedCoExpShare(orderFragmentIn.Volume)
+	orderFragment.MinimumVolume = MarshalEncryptedCoExpShare(orderFragmentIn.MinimumVolume)
+	return orderFragment
+}
+
+func UnmarshalOrderFragment(orderFragmentIn OrderFragment) (ingress.OrderFragment, error) {
 	var err error
 	orderFragment := ingress.OrderFragment{EncryptedFragment: order.EncryptedFragment{}}
 	orderFragment.Index = orderFragmentIn.Index
-	orderFragment.EncryptedFragment.ID, err = adapter.unmarshalOrderFragmentID(orderFragmentIn.ID)
+	orderFragment.EncryptedFragment.ID, err = UnmarshalOrderFragmentID(orderFragmentIn.ID)
 	if err != nil {
 		return orderFragment, err
 	}
-	orderFragment.OrderID, err = adapter.unmarshalOrderID(orderFragmentIn.OrderID)
+	orderFragment.OrderID, err = UnmarshalOrderID(orderFragmentIn.OrderID)
 	if err != nil {
 		return orderFragment, err
 	}
@@ -134,22 +168,22 @@ func (adapter *IngressAdapter) unmarshalOrderFragment(orderFragmentIn OrderFragm
 	if err != nil {
 		return orderFragment, err
 	}
-	orderFragment.Price, err = adapter.unmarshalEncryptedCoExpShare(orderFragmentIn.Price)
+	orderFragment.Price, err = UnmarshalEncryptedCoExpShare(orderFragmentIn.Price)
 	if err != nil {
 		return orderFragment, err
 	}
-	orderFragment.Volume, err = adapter.unmarshalEncryptedCoExpShare(orderFragmentIn.Volume)
+	orderFragment.Volume, err = UnmarshalEncryptedCoExpShare(orderFragmentIn.Volume)
 	if err != nil {
 		return orderFragment, err
 	}
-	orderFragment.MinimumVolume, err = adapter.unmarshalEncryptedCoExpShare(orderFragmentIn.MinimumVolume)
+	orderFragment.MinimumVolume, err = UnmarshalEncryptedCoExpShare(orderFragmentIn.MinimumVolume)
 	if err != nil {
 		return orderFragment, err
 	}
 	return orderFragment, nil
 }
 
-func (adapter *IngressAdapter) unmarshalOrderFragmentMapping(orderFragmentMappingIn OrderFragmentMapping) (order.ID, ingress.OrderFragmentMapping, error) {
+func UnmarshalOrderFragmentMapping(orderFragmentMappingIn OrderFragmentMapping) (order.ID, ingress.OrderFragmentMapping, error) {
 	orderID := order.ID{}
 	orderFragmentMapping := ingress.OrderFragmentMapping{}
 
@@ -158,7 +192,7 @@ func (adapter *IngressAdapter) unmarshalOrderFragmentMapping(orderFragmentMappin
 		var err error
 		foundOrderID := false
 		for _, value := range values {
-			if orderID, err = adapter.unmarshalOrderID(value.OrderID); err != nil {
+			if orderID, err = UnmarshalOrderID(value.OrderID); err != nil {
 				return orderID, orderFragmentMapping, err
 			}
 			foundOrderID = true
@@ -182,7 +216,7 @@ func (adapter *IngressAdapter) unmarshalOrderFragmentMapping(orderFragmentMappin
 		copy(hash[:], hashBytes)
 		orderFragmentMapping[hash] = make([]ingress.OrderFragment, 0, len(orderFragmentsIn))
 		for _, orderFragmentIn := range orderFragmentsIn {
-			orderFragment, err := adapter.unmarshalOrderFragment(orderFragmentIn)
+			orderFragment, err := UnmarshalOrderFragment(orderFragmentIn)
 			if err != nil {
 				return orderID, orderFragmentMapping, err
 			}
diff --git a/http/adapter/ingress_test.go b/http/adapter/ingress_test.go
index eab8756d..256e3c42 100644
--- a/http/adapter/ingress_test.go
+++ b/http/adapter/ingress_test.go
@@ -3,10 +3,13 @@ package adapter_test
 import (
 	"crypto/rand"
 	"encoding/base64"
+	mathRand "math/rand"
 	"sync/atomic"
+	"time"
 
 	. "github.com/onsi/ginkgo"
 	. "github.com/onsi/gomega"
+	"github.com/republicprotocol/republic-go/crypto"
 	. "github.com/republicprotocol/republic-go/http/adapter"
 
 	"github.com/republicprotocol/republic-go/ingress"
@@ -15,17 +18,160 @@ import (
 
 var _ = Describe("Ingress Adapter", func() {
 
+	Context("when marshaling and unmarshaling order fragment mappings", func() {
+
+		var ord order.Order
+		var orderFragmentMappingIn OrderFragmentMapping
+		var podHashBytes [32]byte
+		var podHash string
+
+		BeforeEach(func() {
+			rsaKey, err := crypto.RandomRsaKey()
+			Expect(err).ShouldNot(HaveOccurred())
+			ord, err = createOrder()
+			Expect(err).ShouldNot(HaveOccurred())
+			fragments, err := ord.Split(24, 16)
+			Expect(err).ShouldNot(HaveOccurred())
+
+			signatureBytes := [65]byte{}
+			_, err = rand.Read(signatureBytes[:])
+			Expect(err).ShouldNot(HaveOccurred())
+
+			podHashBytes = [32]byte{}
+			_, err = rand.Read(podHashBytes[:])
+			Expect(err).ShouldNot(HaveOccurred())
+			podHash = base64.StdEncoding.EncodeToString(podHashBytes[:])
+
+			orderFragmentMappingIn = OrderFragmentMapping{}
+			orderFragmentMappingIn[podHash] = []OrderFragment{}
+			for i, fragment := range fragments {
+				orderFragment := ingress.OrderFragment{
+					Index: int64(i),
+				}
+				orderFragment.EncryptedFragment, err = fragment.Encrypt(rsaKey.PublicKey)
+				Expect(err).ShouldNot(HaveOccurred())
+				orderFragmentMappingIn[podHash] = append(
+					orderFragmentMappingIn[podHash],
+					MarshalOrderFragment(orderFragment))
+			}
+		})
+
+		It("should return the same data after marshaling and unmarshaling well formed data", func() {
+			ordID, orderFragmentMapping, err := UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).ShouldNot(HaveOccurred())
+			Expect(ordID).Should(Equal(ord.ID))
+			Expect(orderFragmentMapping).Should(HaveLen(1))
+
+			for i, fragment := range orderFragmentMapping[podHashBytes] {
+				orderFragmentIn := MarshalOrderFragment(fragment)
+				Expect(orderFragmentIn).Should(Equal(orderFragmentMappingIn[podHash][i]))
+			}
+		})
+
+		It("should return an error for malformed order fragment IDs", func() {
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].ID = orderFragmentMappingIn[podHash][i].ID[1:]
+			}
+			_, _, err := UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].ID = "this is invalid"
+			}
+			_, _, err = UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+		})
+
+		It("should return an error for malformed order fragment IDs", func() {
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].OrderID = orderFragmentMappingIn[podHash][i].OrderID[1:]
+			}
+			_, _, err := UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].OrderID = "this is invalid"
+			}
+			_, _, err = UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+		})
+
+		It("should return an error for malformed pod hashes", func() {
+			orderFragmentMappingIn[podHash[16:]] = orderFragmentMappingIn[podHash]
+			_, _, err := UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+
+			delete(orderFragmentMappingIn, podHash[16:])
+			orderFragmentMappingIn["this is invalid"] = orderFragmentMappingIn[podHash]
+			_, _, err = UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+		})
+
+		It("should return an error for malformed tokens", func() {
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].Tokens = "this is invalid"
+			}
+			_, _, err := UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+		})
+
+		It("should return an error for malformed prices", func() {
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].Price = []string{}
+			}
+			_, _, err := UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].Price = []string{"this is invalid", "this is also invalid"}
+			}
+			_, _, err = UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+		})
+
+		It("should return an error for malformed volumes", func() {
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].Volume = []string{}
+			}
+			_, _, err := UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].Volume = []string{"this is invalid", "this is also invalid"}
+			}
+			_, _, err = UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+		})
+
+		It("should return an error for malformed minimum volumes", func() {
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].MinimumVolume = []string{}
+			}
+			_, _, err := UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+
+			for i := range orderFragmentMappingIn[podHash] {
+				orderFragmentMappingIn[podHash][i].MinimumVolume = []string{"this is invalid", "this is also invalid"}
+			}
+			_, _, err = UnmarshalOrderFragmentMapping(orderFragmentMappingIn)
+			Expect(err).Should(HaveOccurred())
+		})
+	})
+
 	Context("when opening orders", func() {
 
-		It("should call ingress.OpenOrder if signature and pool is valid", func() {
+		It("should forward data to the ingress if the signature and mapping are well formed", func() {
 			ingress := &mockIngress{}
 			ingressAdapter := NewIngressAdapter(ingress)
+
 			signatureBytes := [65]byte{}
 			_, err := rand.Read(signatureBytes[:])
 			Expect(err).ShouldNot(HaveOccurred())
 			signature := base64.StdEncoding.EncodeToString(signatureBytes[:])
+
 			orderFragmentMappingIn := OrderFragmentMapping{}
 			orderFragmentMappingIn["Td2YBy0MRYPYqqBduRmDsIhTySQUlMhPBM+wnNPWKqq="] = []OrderFragment{}
+
 			err = ingressAdapter.OpenOrder(signature, orderFragmentMappingIn)
 			Expect(err).ShouldNot(HaveOccurred())
 			Expect(atomic.LoadInt64(&ingress.numOpened)).To(Equal(int64(1)))
@@ -113,3 +259,11 @@ func (ingress *mockIngress) CancelOrder(signature [65]byte, orderID order.ID) er
 func (ingress *mockIngress) Sync() error {
 	return nil
 }
+
+func createOrder() (order.Order, error) {
+	parity := order.ParityBuy
+	price := uint64(mathRand.Intn(2000))
+	volume := uint64(mathRand.Intn(2000))
+	nonce := int64(mathRand.Intn(1000000000))
+	return order.NewOrder(order.TypeLimit, parity, time.Now().Add(time.Hour), order.TokensETHREN, order.NewCoExp(price, 26), order.NewCoExp(volume, 26), order.NewCoExp(volume, 26), nonce), nil
+}
diff --git a/ingress/ingress.go b/ingress/ingress.go
index a8d0664d..def206d9 100644
--- a/ingress/ingress.go
+++ b/ingress/ingress.go
@@ -234,6 +234,12 @@ func (ingress *ingress) sendOrderFragmentsToPod(pod cal.Pod, orderFragments []Or
 	go func() {
 		defer close(errs)
 
+		fmtStr := "[pod = %v] sending order = " + base64.StdEncoding.EncodeToString(pod.Hash[:]) + "\n"
+		for _, darknode := range pod.Darknodes {
+			fmtStr += "  sending order fragment to " + darknode.String() + "\n"
+		}
+		log.Printf(fmtStr)
+
 		dispatch.CoForAll(pod.Darknodes, func(i int) {
 			orderFragment, ok := orderFragmentIndexMapping[int64(i)]
 			if !ok {
@@ -251,7 +257,6 @@ func (ingress *ingress) sendOrderFragmentsToPod(pod cal.Pod, orderFragments []Or
 				return
 			}
 
-			log.Printf("sending order fragment to %v", darknode)
 			if err := ingress.orderbookClient.OpenOrder(ctx, darknodeMultiAddr, orderFragment.EncryptedFragment); err != nil {
 				log.Printf("cannot send order fragment to %v: %v", darknode, err)
 				errs <- fmt.Errorf("cannot send order fragment to %v: %v", darknode, err)
diff --git a/ingress/ingress_test.go b/ingress/ingress_test.go
index 15420e20..e92564f3 100644
--- a/ingress/ingress_test.go
+++ b/ingress/ingress_test.go
@@ -8,6 +8,7 @@ import (
 	"fmt"
 	"math/big"
 	mathRand "math/rand"
+	"sync"
 	"time"
 
 	. "github.com/onsi/ginkgo"
@@ -276,16 +277,21 @@ func (darkpool *mockDarkpool) IsRegistered(addr identity.Address) (bool, error)
 }
 
 type mockRenLedger struct {
+	mu          *sync.Mutex
 	orderStates map[string]struct{}
 }
 
 func newMockRenLedger() mockRenLedger {
 	return mockRenLedger{
+		mu:          new(sync.Mutex),
 		orderStates: map[string]struct{}{},
 	}
 }
 
 func (renLedger *mockRenLedger) OpenBuyOrder(signature [65]byte, orderID order.ID) error {
+	renLedger.mu.Lock()
+	defer renLedger.mu.Unlock()
+
 	if _, ok := renLedger.orderStates[string(orderID[:])]; !ok {
 		renLedger.orderStates[string(orderID[:])] = struct{}{}
 		return nil
@@ -294,6 +300,9 @@ func (renLedger *mockRenLedger) OpenBuyOrder(signature [65]byte, orderID order.I
 }
 
 func (renLedger *mockRenLedger) OpenSellOrder(signature [65]byte, orderID order.ID) error {
+	renLedger.mu.Lock()
+	defer renLedger.mu.Unlock()
+
 	if _, ok := renLedger.orderStates[string(orderID[:])]; !ok {
 		renLedger.orderStates[string(orderID[:])] = struct{}{}
 		return nil
@@ -302,6 +311,9 @@ func (renLedger *mockRenLedger) OpenSellOrder(signature [65]byte, orderID order.
 }
 
 func (renLedger *mockRenLedger) CancelOrder(signature [65]byte, orderID order.ID) error {
+	renLedger.mu.Lock()
+	defer renLedger.mu.Unlock()
+
 	if _, ok := renLedger.orderStates[string(orderID[:])]; ok {
 		delete(renLedger.orderStates, string(orderID[:]))
 		return nil
@@ -310,6 +322,9 @@ func (renLedger *mockRenLedger) CancelOrder(signature [65]byte, orderID order.ID
 }
 
 func (renLedger *mockRenLedger) ConfirmOrder(id order.ID, match order.ID) error {
+	renLedger.mu.Lock()
+	defer renLedger.mu.Unlock()
+
 	if _, ok := renLedger.orderStates[string(id[:])]; ok {
 		delete(renLedger.orderStates, string(id[:]))
 		matches := []order.ID{match}
@@ -326,6 +341,9 @@ func (renLedger *mockRenLedger) ConfirmOrder(id order.ID, match order.ID) error
 }
 
 func (renLedger *mockRenLedger) Fee() (*big.Int, error) {
+	renLedger.mu.Lock()
+	defer renLedger.mu.Unlock()
+
 	return big.NewInt(0), nil
 }
 
diff --git a/logger/file_plugin.go b/logger/file_plugin.go
index b810663c..2344828d 100644
--- a/logger/file_plugin.go
+++ b/logger/file_plugin.go
@@ -76,7 +76,7 @@ func (plugin *FilePlugin) Log(l Log) error {
 			tag = "{" + strings.Join(tags, ",") + "} "
 		}
 
-		_, err := plugin.file.WriteString(fmt.Sprintf("%s [%s] (%s) %s%s\n", l.Timestamp.Format("2006/01/02 15:04:05"), l.Type, l.EventType, tag, l.Event.String()))
+		_, err := plugin.file.WriteString(fmt.Sprintf("%s [%s] (%s) %s%s\n", l.Timestamp.Format("2006/01/02 15:04:05"), l.Level, l.EventType, tag, l.Event.String()))
 		return err
 	}
 	return json.NewEncoder(plugin.file).Encode(l)
diff --git a/logger/file_plugin_test.go b/logger/file_plugin_test.go
index 90c66f62..709f5c03 100644
--- a/logger/file_plugin_test.go
+++ b/logger/file_plugin_test.go
@@ -1 +1 @@
-package logger
+package logger_test
diff --git a/logger/logger.go b/logger/logger.go
index b500696e..b9d39b1f 100644
--- a/logger/logger.go
+++ b/logger/logger.go
@@ -11,13 +11,12 @@ import (
 )
 
 var defaultLoggerMu = new(sync.RWMutex)
-
-// DefaultLogger is used when no Logger object is used to log events.
-var DefaultLogger = func() *Logger {
+var defaultLogger = func() *Logger {
 	logger, err := NewLogger(Options{
 		Plugins: []PluginOptions{
 			PluginOptions{File: &FilePluginOptions{Path: "stdout"}, WebSocket: nil},
 		},
+		FilterLevel: LevelWarn,
 	})
 	if err != nil {
 		panic(fmt.Sprintf("cannot init default logger: %v", err))
@@ -26,96 +25,115 @@ var DefaultLogger = func() *Logger {
 	return logger
 }()
 
+// SetFilterLevel changes the logging level of the defaultLogger.
+func SetFilterLevel(l Level) {
+	defaultLoggerMu.Lock()
+	defer defaultLoggerMu.Unlock()
+	defaultLogger.FilterLevel = l
+}
+
+// SetFilterEvents sets the defaultLogger to filter specific EventTypes.
+func SetFilterEvents(events []EventType) {
+	defaultLoggerMu.Lock()
+	defer defaultLoggerMu.Unlock()
+	defaultLogger.FilterEvents = eventListToMap(events)
+}
+
 // SetDefaultLogger to a specific Logger object. The previous DefaultLogger
 // will be stopped, and the new DefaultLogger will be started.
 func SetDefaultLogger(logger *Logger) {
 	defaultLoggerMu.Lock()
-	defer defaultLoggerMu.Lock()
-	DefaultLogger.Stop()
-	DefaultLogger = logger
+	defer defaultLoggerMu.Unlock()
+	defaultLogger.Stop()
+	defaultLogger = logger
+	defaultLogger.Start()
 }
 
-// LogInfo logs an info Log using a GenericEvent using the DefaultLogger.
-func LogInfo(message string) {
+// Info logs an info Log using a GenericEvent using the DefaultLogger.
+func Info(message string) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.Info(message)
+	defaultLogger.Info(message)
 }
 
-// LogWarn logs a warn Log using a GenericEvent using the DefaultLogger.
-func LogWarn(message string) {
+// Warn logs a warn Log using a GenericEvent using the DefaultLogger.
+func Warn(message string) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.Warn(message)
+	defaultLogger.Warn(message)
 }
 
-// LogError logs an error Log using a GenericEvent using the DefaultLogger.
-func LogError(message string) {
+// Error logs an error Log using a GenericEvent using the DefaultLogger.
+func Error(message string) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.Error(message)
+	defaultLogger.Error(message)
 }
 
-// LogUsage logs an info Log using a UsageEvent using the DefaultLogger.
-func LogUsage(cpu, memory float64, network uint64) {
+// Usage logs an info Log using a UsageEvent using the DefaultLogger.
+func Usage(cpu, memory float64, network uint64) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.Usage(cpu, memory, network)
+	defaultLogger.Usage(cpu, memory, network)
 }
 
-// LogOrderConfirmed logs an OrderConfirmedEvent using the DefaultLogger.
-func LogOrderConfirmed(ty Level, orderID string) {
+// OrderConfirmed logs an OrderConfirmedEvent using the DefaultLogger.
+func OrderConfirmed(ty Level, orderID string) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.OrderConfirmed(ty, orderID)
+	defaultLogger.OrderConfirmed(ty, orderID)
 }
 
-// LogOrderMatch logs an OrderMatchEvent using the DefaultLogger.
-func LogOrderMatch(ty Level, id, buyID, sellID string) {
+// OrderMatch logs an OrderMatchEvent using the DefaultLogger.
+func OrderMatch(ty Level, id, buyID, sellID string) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.OrderMatch(ty, id, buyID, sellID)
+	defaultLogger.OrderMatch(ty, id, buyID, sellID)
 }
 
-// LogBuyOrderReceived logs an OrderReceivedEvent using the DefaultLogger.
-func LogBuyOrderReceived(ty Level, id, fragmentID string) {
+// BuyOrderReceived logs an OrderReceivedEvent using the DefaultLogger.
+func BuyOrderReceived(ty Level, id, fragmentID string) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.BuyOrderReceived(ty, id, fragmentID)
+	defaultLogger.BuyOrderReceived(ty, id, fragmentID)
 }
 
-// LogSellOrderReceived logs an OrderReceivedEvent using the DefaultLogger.
-func LogSellOrderReceived(ty Level, id, fragmentID string) {
+// SellOrderReceived logs an OrderReceivedEvent using the DefaultLogger.
+func SellOrderReceived(ty Level, id, fragmentID string) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.SellOrderReceived(ty, id, fragmentID)
+	defaultLogger.SellOrderReceived(ty, id, fragmentID)
 }
 
-// LogNetwork logs a NetworkEvent using the DefaultLogger.
-func LogNetwork(ty Level, message string) {
+// Network logs a NetworkEvent using the DefaultLogger.
+func Network(ty Level, message string) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.Network(ty, message)
+	defaultLogger.Network(ty, message)
 }
 
-// LogCompute logs a ComputeEvent using the DefaultLogger.
-func LogCompute(ty Level, message string) {
+// Compute logs a ComputeEvent using the DefaultLogger.
+func Compute(ty Level, message string) {
 	defaultLoggerMu.Lock()
 	defer defaultLoggerMu.Unlock()
-	DefaultLogger.Compute(ty, message)
+	defaultLogger.Compute(ty, message)
 }
 
 // Logger handles distributing logs to plugins registered with it
 type Logger struct {
 	do.GuardedObject
-	Plugins []Plugin
-	Tags    map[string]string
+	Plugins      []Plugin
+	Tags         map[string]string
+	FilterLevel  Level
+	FilterEvents map[EventType]struct{}
 }
 
 // Options are used to Unmarshal a Logger from JSON.
 type Options struct {
-	Plugins []PluginOptions   `json:"plugins"`
-	Tags    map[string]string `json:"tags"`
+	Plugins      []PluginOptions   `json:"plugins"`
+	Tags         map[string]string `json:"tags"`
+	FilterLevel  Level             `json:"filterLevel"`
+	FilterEvents []EventType       `json:"filterEvents"`
 }
 
 // The Plugin interface describes a worker that consumes logs
@@ -131,12 +149,23 @@ type PluginOptions struct {
 	WebSocket *WebSocketPluginOptions `json:"websocket,omitempty"`
 }
 
+func eventListToMap(events []EventType) map[EventType]struct{} {
+	// Convert the events from an array into a map for O(1) lookup
+	eventMap := make(map[EventType]struct{})
+	for _, event := range events {
+		eventMap[event] = struct{}{}
+	}
+	return eventMap
+}
+
 // NewLogger returns a new Logger that will start and stop a set of plugins.
 func NewLogger(options Options) (*Logger, error) {
 	logger := &Logger{
 		GuardedObject: do.NewGuardedObject(),
 		Plugins:       make([]Plugin, 0, len(options.Plugins)),
 		Tags:          options.Tags,
+		FilterLevel:   options.FilterLevel,
+		FilterEvents:  eventListToMap(options.FilterEvents),
 	}
 	for i := range options.Plugins {
 		if options.Plugins[i].File != nil {
@@ -174,20 +203,25 @@ func (logger Logger) Stop() {
 
 // Log an Event.
 func (logger *Logger) Log(l Log) {
-	l.Tags = logger.Tags
-	for _, plugin := range logger.Plugins {
-		if err := plugin.Log(l); err != nil {
-			log.Println(err)
+	if _, ok := logger.FilterEvents[l.EventType]; !ok && len(logger.FilterEvents) > 0 {
+		return
+	}
+	if l.Level <= logger.FilterLevel {
+		l.Tags = logger.Tags
+		for _, plugin := range logger.Plugins {
+			if err := plugin.Log(l); err != nil {
+				log.Println(err)
+			}
 		}
 	}
 }
 
-// Info logs an info Log using a GenericEvent.
-func (logger *Logger) Info(message string) {
+// Error logs an error Log using a GenericEvent.
+func (logger *Logger) Error(message string) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      LevelInfo,
-		EventType: Generic,
+		Level:     LevelError,
+		EventType: TypeGeneric,
 		Event: GenericEvent{
 			Message: message,
 		},
@@ -198,20 +232,56 @@ func (logger *Logger) Info(message string) {
 func (logger *Logger) Warn(message string) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      Warn,
-		EventType: Generic,
+		Level:     LevelWarn,
+		EventType: TypeGeneric,
 		Event: GenericEvent{
 			Message: message,
 		},
 	})
 }
 
-// Error logs an error Log using a GenericEvent.
-func (logger *Logger) Error(message string) {
+// Info logs an info Log using a GenericEvent.
+func (logger *Logger) Info(message string) {
+	logger.Log(Log{
+		Timestamp: time.Now(),
+		Level:     LevelInfo,
+		EventType: TypeGeneric,
+		Event: GenericEvent{
+			Message: message,
+		},
+	})
+}
+
+// DebugHigh logs a debug Log using a GenericEvent.
+func (logger *Logger) DebugHigh(message string) {
+	logger.Log(Log{
+		Timestamp: time.Now(),
+		Level:     LevelDebugHigh,
+		EventType: TypeGeneric,
+		Event: GenericEvent{
+			Message: message,
+		},
+	})
+}
+
+// Debug logs a debug Log using a GenericEvent.
+func (logger *Logger) Debug(message string) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      Error,
-		EventType: Generic,
+		Level:     LevelDebug,
+		EventType: TypeGeneric,
+		Event: GenericEvent{
+			Message: message,
+		},
+	})
+}
+
+// DebugLow logs a debug Log using a GenericEvent.
+func (logger *Logger) DebugLow(message string) {
+	logger.Log(Log{
+		Timestamp: time.Now(),
+		Level:     LevelDebugLow,
+		EventType: TypeGeneric,
 		Event: GenericEvent{
 			Message: message,
 		},
@@ -222,8 +292,8 @@ func (logger *Logger) Error(message string) {
 func (logger *Logger) Usage(cpu, memory float64, network uint64) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      LevelInfo,
-		EventType: Usage,
+		Level:     LevelInfo,
+		EventType: TypeUsage,
 		Event: UsageEvent{
 			CPU:     cpu,
 			Memory:  memory,
@@ -236,8 +306,8 @@ func (logger *Logger) Usage(cpu, memory float64, network uint64) {
 func (logger *Logger) OrderConfirmed(ty Level, orderID string) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      ty,
-		EventType: OrderConfirmed,
+		Level:     ty,
+		EventType: TypeOrderConfirmed,
 		Event: OrderConfirmedEvent{
 			OrderID: orderID,
 		},
@@ -248,8 +318,8 @@ func (logger *Logger) OrderConfirmed(ty Level, orderID string) {
 func (logger *Logger) OrderMatch(ty Level, id, buyID, sellID string) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      ty,
-		EventType: OrderMatch,
+		Level:     ty,
+		EventType: TypeOrderMatch,
 		Event: OrderMatchEvent{
 			ID:     id,
 			BuyID:  buyID,
@@ -262,8 +332,8 @@ func (logger *Logger) OrderMatch(ty Level, id, buyID, sellID string) {
 func (logger *Logger) BuyOrderReceived(ty Level, id, fragmentID string) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      ty,
-		EventType: OrderReceived,
+		Level:     ty,
+		EventType: TypeOrderReceived,
 		Event: OrderReceivedEvent{
 			BuyID:      &id,
 			FragmentID: fragmentID,
@@ -275,8 +345,8 @@ func (logger *Logger) BuyOrderReceived(ty Level, id, fragmentID string) {
 func (logger *Logger) SellOrderReceived(ty Level, id, fragmentID string) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      ty,
-		EventType: OrderReceived,
+		Level:     ty,
+		EventType: TypeOrderReceived,
 		Event: OrderReceivedEvent{
 			SellID:     &id,
 			FragmentID: fragmentID,
@@ -288,8 +358,8 @@ func (logger *Logger) SellOrderReceived(ty Level, id, fragmentID string) {
 func (logger *Logger) Network(ty Level, message string) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      ty,
-		EventType: Network,
+		Level:     ty,
+		EventType: TypeNetwork,
 		Event: NetworkEvent{
 			Message: message,
 		},
@@ -300,8 +370,8 @@ func (logger *Logger) Network(ty Level, message string) {
 func (logger *Logger) Compute(ty Level, message string) {
 	logger.Log(Log{
 		Timestamp: time.Now(),
-		Type:      ty,
-		EventType: Compute,
+		Level:     ty,
+		EventType: TypeCompute,
 		Event: ComputeEvent{
 			Message: message,
 		},
@@ -309,36 +379,52 @@ func (logger *Logger) Compute(ty Level, message string) {
 }
 
 // Level defines the different levels of Log messages that can be sent.
-type Level string
+type Level uint8
 
 // Values for the LogType.
 const (
-	LevelInfo = Level("info")
-	Warn      = Level("warn")
-	Error     = Level("error")
+	LevelError     = Level(1)
+	LevelWarn      = Level(2)
+	LevelInfo      = Level(3)
+	LevelDebugHigh = Level(4)
+	LevelDebug     = Level(5)
+	LevelDebugLow  = Level(6)
 )
 
+func (level Level) String() string {
+	switch level {
+	case LevelError:
+		return "error"
+	case LevelWarn:
+		return "warn"
+	case LevelInfo:
+		return "info"
+	default:
+		return "debug"
+	}
+}
+
 // EventType defines the different types of Event messages that can be sent in a
 // Log.
 type EventType string
 
 // Values for the EventType.
 const (
-	Generic        = EventType("generic")
-	Epoch          = EventType("epoch")
-	Usage          = EventType("usage")
-	Ethereum       = EventType("ethereum")
-	OrderConfirmed = EventType("orderConfirmed")
-	OrderMatch     = EventType("orderMatch")
-	OrderReceived  = EventType("orderReceived")
-	Network        = EventType("network")
-	Compute        = EventType("compute")
+	TypeGeneric        = EventType("generic")
+	TypeEpoch          = EventType("epoch")
+	TypeUsage          = EventType("usage")
+	TypeEthereum       = EventType("ethereum")
+	TypeOrderConfirmed = EventType("orderConfirmed")
+	TypeOrderMatch     = EventType("orderMatch")
+	TypeOrderReceived  = EventType("orderReceived")
+	TypeNetwork        = EventType("network")
+	TypeCompute        = EventType("compute")
 )
 
 // A Log is logged by the Logger using all available Plugins.
 type Log struct {
 	Timestamp time.Time         `json:"timestamp"`
-	Type      Level             `json:"type"`
+	Level     Level             `json:"type"`
 	EventType EventType         `json:"eventType"`
 	Event     Event             `json:"event"`
 	Tags      map[string]string `json:"tags"`
diff --git a/logger/websocket_plugin_test.go b/logger/websocket_plugin_test.go
index 90c66f62..709f5c03 100644
--- a/logger/websocket_plugin_test.go
+++ b/logger/websocket_plugin_test.go
@@ -1 +1 @@
-package logger
+package logger_test
diff --git a/ome/computer.go b/ome/computer.go
index b6dd1672..ad4f6279 100644
--- a/ome/computer.go
+++ b/ome/computer.go
@@ -25,13 +25,6 @@ type Computation struct {
 	Priority Priority
 }
 
-type State uint64
-
-const (
-	StatePending   = 0
-	StateComputing = 1
-)
-
 // TODO: Stage bytes are a really ugly way of tracking our computations. We
 // need a proper SMPC VM.
 type Stage byte
@@ -243,19 +236,14 @@ func (computer *computer) processComputation(computation ComputationEpoch, pendi
 	buy, err := computer.storer.OrderFragment(computation.Buy)
 	if err != nil {
 		pendingComputations[computation.ID] = computation
-		log.Printf("cannot find buy order %v", base64.StdEncoding.EncodeToString(computation.Buy[:]))
 		return
 	}
 	sell, err := computer.storer.OrderFragment(computation.Sell)
 	if err != nil {
-		log.Printf("cannot find sell order %v", base64.StdEncoding.EncodeToString(computation.Buy[:]))
 		pendingComputations[computation.ID] = computation
 		return
 	}
 
-	log.Println(buy)
-	log.Println(sell)
-
 	delete(pendingComputations, computation.ID)
 	var share shamir.Share
 	switch computation.ID[31] {
@@ -312,22 +300,24 @@ func (computer *computer) processComputation(computation ComputationEpoch, pendi
 		},
 	}
 
-	log.Printf("sending computation: buy = %v; sell = %v", base64.StdEncoding.EncodeToString(computation.Buy[:]), base64.StdEncoding.EncodeToString(computation.Sell[:]))
-
-	invertingFlow := true
-	for invertingFlow {
-		select {
-		case <-done:
-			invertingFlow = false
-		case insts <- inst:
-			invertingFlow = false
-		case computation, ok := <-computer.computations:
-			if !ok {
-				invertingFlow = false
-				break
-			}
-			pendingComputations[computation.ID] = computation
+	log.Printf("[stage => %v] processing computation: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+
+	// Write instruction
+	select {
+	case insts <- inst:
+	default:
+	}
+
+	// Invert flow if necessary
+	select {
+	case <-done:
+	case insts <- inst:
+	case computation, ok := <-computer.computations:
+		log.Printf("compute inversion: buy = %v; sell = %v", base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+		if !ok {
+			break
 		}
+		pendingComputations[computation.ID] = computation
 	}
 }
 
@@ -360,47 +350,74 @@ func (computer *computer) processResultJ(instID, networkID [32]byte, resultJ smp
 	if !ok {
 		return
 	}
-	log.Printf("last byte is %v", computation.ID[31])
+
+	log.Printf("[stage => %v] received result: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:]), base64.StdEncoding.EncodeToString(computation.Sell[:]))
 
 	switch instID[31] {
+
 	case StageCmpPriceExp:
-		if resultJ.Value <= half {
-			computation.ID[31] = StageCmpPriceCo
+		if resultJ.Value > half {
+			log.Printf("[stage => %v] halt: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+			return
 		}
+		log.Printf("[stage => %v] ok: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+		computation.ID[31] = StageCmpPriceCo
+
 	case StageCmpPriceCo:
-		if resultJ.Value <= half {
-			computation.ID[31] = StageCmpBuyVolExp
+		if resultJ.Value > half {
+			log.Printf("[stage => %v] halt: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+			return
 		}
+		log.Printf("[stage => %v] ok: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+		computation.ID[31] = StageCmpBuyVolExp
+
 	case StageCmpBuyVolExp:
-		if resultJ.Value <= half {
-			computation.ID[31] = StageCmpBuyVolCo
+		if resultJ.Value > half {
+			log.Printf("[stage => %v] halt: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+			return
 		}
+		log.Printf("[stage => %v] ok: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+		computation.ID[31] = StageCmpBuyVolCo
+
 	case StageCmpBuyVolCo:
-		if resultJ.Value <= half {
-			computation.ID[31] = StageCmpSellVolExp
+		if resultJ.Value > half {
+			log.Printf("[stage => %v] halt: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+			return
 		}
+		log.Printf("[stage => %v] ok: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+		computation.ID[31] = StageCmpSellVolExp
+
 	case StageCmpSellVolExp:
-		if resultJ.Value <= half {
-			computation.ID[31] = StageCmpSellVolCo
+		if resultJ.Value > half {
+			log.Printf("[stage => %v] halt: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+			return
 		}
+		log.Printf("[stage => %v] ok: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+		computation.ID[31] = StageCmpSellVolCo
+
 	case StageCmpSellVolCo:
-		if resultJ.Value <= half {
-			computation.ID[31] = StageCmpTokens
+		if resultJ.Value > half {
+			log.Printf("[stage => %v] halt: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+			return
 		}
+		log.Printf("[stage => %v] ok: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
+		computation.ID[31] = StageCmpTokens
+
 	case StageCmpTokens:
 		if resultJ.Value == 0 {
 			computation.ID = computeID(computation.Computation)
-
 			computer.matchingComputationsMu.Lock()
 			computer.matchingComputationsState[computation.ID] = computation
 			computer.matchingComputationsMu.Unlock()
 			select {
 			case <-done:
 			case computer.matchingComputations <- computation.Computation:
-				log.Print("------------match found--------------")
+				log.Printf("✔ [stage => %v] matched: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
 			}
-			return
+		} else {
+			log.Printf("[stage => %v] halt: buy = %v; sell = %v", computation.ID[31], base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
 		}
+		return
 
 	case StageJoinBuyPriceExp:
 		computer.priceExpPointer[computation.Buy] = &resultJ.Value
diff --git a/ome/ome.go b/ome/ome.go
index f1fdd6ae..9be7a20a 100644
--- a/ome/ome.go
+++ b/ome/ome.go
@@ -134,17 +134,17 @@ func (ome *ome) Run(done <-chan struct{}) <-chan error {
 				case <-done:
 					return
 				case computations <- computation:
-					log.Printf("new computation received , buy: %v, sell: %v", base64.StdEncoding.EncodeToString(computation.Buy[:]), base64.StdEncoding.EncodeToString(computation.Sell[:]))
+					log.Printf("new computation: buy = %v; sell = %v", base64.StdEncoding.EncodeToString(computation.Buy[:8]), base64.StdEncoding.EncodeToString(computation.Sell[:8]))
 				}
 			}
 			if n == 128 {
 				continue
 			}
 
-			if time.Now().After(syncStart.Add(4 * time.Second)) {
+			if time.Now().After(syncStart.Add(14 * time.Second)) {
 				continue
 			}
-			time.Sleep(syncStart.Add(4 * time.Second).Sub(time.Now()))
+			time.Sleep(syncStart.Add(14 * time.Second).Sub(time.Now()))
 		}
 	}()
 
diff --git a/ome/ranker.go b/ome/ranker.go
index 21aae1c6..002b667c 100644
--- a/ome/ranker.go
+++ b/ome/ranker.go
@@ -88,13 +88,9 @@ func (ranker *ranker) InsertBuy(order PriorityOrder) {
 		index := sort.Search(len(ranker.computations), func(i int) bool {
 			return ranker.computations[i].Priority > computation.Priority
 		})
-		ranker.computations = append(
-			append(
-				ranker.computations[:index],
-				computation,
-			),
-			ranker.computations[index:]...,
-		)
+		ranker.computations = append(ranker.computations, Computation{})
+		copy(ranker.computations[index+1:], ranker.computations[index:])
+		ranker.computations[index] = computation
 	}
 }
 
@@ -119,13 +115,9 @@ func (ranker *ranker) InsertSell(order PriorityOrder) {
 		index := sort.Search(len(ranker.computations), func(i int) bool {
 			return ranker.computations[i].Priority > computation.Priority
 		})
-		ranker.computations = append(
-			append(
-				ranker.computations[:index],
-				computation,
-			),
-			ranker.computations[index:]...,
-		)
+		ranker.computations = append(ranker.computations, Computation{})
+		copy(ranker.computations[index+1:], ranker.computations[index:])
+		ranker.computations[index] = computation
 	}
 }
 
diff --git a/order/fragment.go b/order/fragment.go
index f83e6133..7b82a265 100644
--- a/order/fragment.go
+++ b/order/fragment.go
@@ -57,10 +57,11 @@ func (fragment *Fragment) Hash() [32]byte {
 }
 
 // Bytes returns a Fragment serialized into a bytes.
+// TODO: This function should return an error.
 func (fragment *Fragment) Bytes() []byte {
 	buf := new(bytes.Buffer)
-	binary.Write(buf, binary.BigEndian, fragment.OrderID)
 	binary.Write(buf, binary.BigEndian, fragment.OrderType)
+	binary.Write(buf, binary.BigEndian, fragment.OrderID)
 	binary.Write(buf, binary.BigEndian, fragment.OrderParity)
 	binary.Write(buf, binary.BigEndian, fragment.OrderExpiry.Unix())
 	binary.Write(buf, binary.BigEndian, fragment.Tokens)
diff --git a/order/fragment_test.go b/order/fragment_test.go
index 9d50a323..c7864b64 100644
--- a/order/fragment_test.go
+++ b/order/fragment_test.go
@@ -131,7 +131,7 @@ var _ = Describe("Order fragments", func() {
 
 	Context("when encrypting and decrypting fragments", func() {
 
-		It("should return the same fragment after decrypting it's encrypted form", func() {
+		It("should return the same fragment after decrypting its encrypted form", func() {
 			copy(orderID[:], "orderID")
 			fragment := NewFragment(orderID, TypeLimit, ParityBuy, tokens, price, maxVolume, minVolume)
 
diff --git a/order/order.go b/order/order.go
index 2769c9a4..2c51aed7 100644
--- a/order/order.go
+++ b/order/order.go
@@ -230,6 +230,7 @@ func (order *Order) Hash() [32]byte {
 }
 
 // Bytes returns an Order serialized into a bytes.
+// TODO: This function should return an error.
 func (order *Order) Bytes() []byte {
 	buf := new(bytes.Buffer)
 	binary.Write(buf, binary.BigEndian, order.Type)
diff --git a/order/value.go b/order/value.go
index 351b73ba..bf8ca8c8 100644
--- a/order/value.go
+++ b/order/value.go
@@ -57,8 +57,12 @@ func (val *CoExp) UnmarshalJSON(data []byte) error {
 // MarshalBinary implements the encoding.BinaryMarshaler interface.
 func (val CoExp) MarshalBinary() (data []byte, err error) {
 	buf := new(bytes.Buffer)
-	binary.Write(buf, binary.BigEndian, val.Co)
-	binary.Write(buf, binary.BigEndian, val.Exp)
+	if err := binary.Write(buf, binary.BigEndian, val.Co); err != nil {
+		return nil, err
+	}
+	if err := binary.Write(buf, binary.BigEndian, val.Exp); err != nil {
+		return nil, err
+	}
 	return buf.Bytes(), nil
 }
 
@@ -199,8 +203,12 @@ func (val *EncryptedCoExpShare) UnmarshalJSON(data []byte) error {
 // marshals the EncryptedCoExpShare using encoding.BigEndian.
 func (val EncryptedCoExpShare) MarshalBinary() ([]byte, error) {
 	buf := new(bytes.Buffer)
-	binary.Write(buf, binary.BigEndian, val.Co)
-	binary.Write(buf, binary.BigEndian, val.Exp)
+	if err := binary.Write(buf, binary.BigEndian, val.Co); err != nil {
+		return nil, err
+	}
+	if err := binary.Write(buf, binary.BigEndian, val.Exp); err != nil {
+		return nil, err
+	}
 	return buf.Bytes(), nil
 }
 
diff --git a/orderbook/orderbook_test.go b/orderbook/orderbook_test.go
index 84d33b65..12781576 100644
--- a/orderbook/orderbook_test.go
+++ b/orderbook/orderbook_test.go
@@ -7,9 +7,9 @@ import (
 
 	. "github.com/onsi/ginkgo"
 	. "github.com/onsi/gomega"
-	"github.com/republicprotocol/republic-go/crypto"
 	. "github.com/republicprotocol/republic-go/orderbook"
 
+	"github.com/republicprotocol/republic-go/crypto"
 	"github.com/republicprotocol/republic-go/order"
 )
 
diff --git a/orderbook/sync.go b/orderbook/sync.go
index 41181a02..af7c754b 100644
--- a/orderbook/sync.go
+++ b/orderbook/sync.go
@@ -3,6 +3,7 @@ package orderbook
 import (
 	"fmt"
 	"log"
+	"sync"
 
 	"github.com/republicprotocol/republic-go/cal"
 	"github.com/republicprotocol/republic-go/dispatch"
@@ -40,8 +41,10 @@ type syncer struct {
 	renLedgerLimit   int
 	buyOrderPointer  int
 	sellOrderPointer int
-	buyOrders        map[int]order.ID
-	sellOrders       map[int]order.ID
+
+	ordersMu   *sync.RWMutex
+	buyOrders  map[int]order.ID
+	sellOrders map[int]order.ID
 }
 
 func NewSyncer(renLedger cal.RenLedger, limit int) Syncer {
@@ -50,6 +53,7 @@ func NewSyncer(renLedger cal.RenLedger, limit int) Syncer {
 		renLedgerLimit:   limit,
 		buyOrderPointer:  0,
 		sellOrderPointer: 0,
+		ordersMu:         new(sync.RWMutex),
 		buyOrders:        map[int]order.ID{},
 		sellOrders:       map[int]order.ID{},
 	}
@@ -64,6 +68,7 @@ func (syncer *syncer) Sync() (ChangeSet, error) {
 		for i, ord := range buyOrderIDs {
 			change := NewChange(ord, order.ParityBuy, order.Open, uint64(syncer.buyOrderPointer+i))
 			changeset = append(changeset, change)
+			syncer.buyOrders[syncer.buyOrderPointer+i] = ord
 		}
 	}
 
@@ -74,6 +79,7 @@ func (syncer *syncer) Sync() (ChangeSet, error) {
 		for i, ord := range sellOrderIDs {
 			change := NewChange(ord, order.ParitySell, order.Open, uint64(syncer.sellOrderPointer+i))
 			changeset = append(changeset, change)
+			syncer.sellOrders[syncer.sellOrderPointer+i] = ord
 		}
 	}
 	if buyErr != nil && sellErr != nil {
@@ -93,38 +99,56 @@ func (syncer *syncer) purge() ChangeSet {
 				// Purge all buy orders by iterating over them and reading
 				// their status and priority from the Ren Ledger
 				dispatch.CoForAll(syncer.buyOrders, func(key int) {
-					status, err := syncer.renLedger.Status(syncer.buyOrders[key])
+					syncer.ordersMu.RLock()
+					buyOrder := syncer.buyOrders[key]
+					syncer.ordersMu.RUnlock()
+
+					status, err := syncer.renLedger.Status(buyOrder)
 					if err != nil {
 						log.Println("fail to check order status", err)
 						return
 					}
-					priority, err := syncer.renLedger.Priority(syncer.buyOrders[key])
+
+					priority, err := syncer.renLedger.Priority(buyOrder)
 					if err != nil {
 						log.Println("fail to check order priority", err)
 						return
 					}
+
 					if status != order.Open {
-						changes <- NewChange(syncer.buyOrders[key], order.ParityBuy, status, priority)
+						changes <- NewChange(buyOrder, order.ParityBuy, status, priority)
+
+						syncer.ordersMu.Lock()
 						delete(syncer.buyOrders, key)
+						syncer.ordersMu.Unlock()
 					}
 				})
 			},
 			func() {
 				// Purge all sell orders
 				dispatch.CoForAll(syncer.sellOrders, func(key int) {
-					status, err := syncer.renLedger.Status(syncer.sellOrders[key])
+					syncer.ordersMu.RLock()
+					sellOrder := syncer.sellOrders[key]
+					syncer.ordersMu.RUnlock()
+
+					status, err := syncer.renLedger.Status(sellOrder)
 					if err != nil {
 						log.Println("fail to check order status", err)
 						return
 					}
-					priority, err := syncer.renLedger.Priority(syncer.sellOrders[key])
+
+					priority, err := syncer.renLedger.Priority(sellOrder)
 					if err != nil {
 						log.Println("fail to check order priority", err)
 						return
 					}
+
 					if status != order.Open {
-						changes <- NewChange(syncer.sellOrders[key], order.ParitySell, status, priority)
+						changes <- NewChange(sellOrder, order.ParitySell, status, priority)
+
+						syncer.ordersMu.Lock()
 						delete(syncer.sellOrders, key)
+						syncer.ordersMu.Unlock()
 					}
 				})
 			},
diff --git a/orderbook/sync_test.go b/orderbook/sync_test.go
index c4cf7545..3b5c642e 100644
--- a/orderbook/sync_test.go
+++ b/orderbook/sync_test.go
@@ -1 +1,258 @@
 package orderbook_test
+
+import (
+	"encoding/binary"
+	"errors"
+	"fmt"
+	"math/big"
+	"math/rand"
+	"sync"
+	"time"
+
+	. "github.com/onsi/ginkgo"
+	. "github.com/onsi/gomega"
+	. "github.com/republicprotocol/republic-go/orderbook"
+
+	"github.com/republicprotocol/republic-go/order"
+)
+
+var ErrCannotFindOrder = errors.New("cannot find order")
+
+var _ = Describe("Syncer", func() {
+
+	Context("when syncing with the ledger", func() {
+
+		It("should sync with renledger by updating memory and returning a ChangeSet", func() {
+			numberOfOrderPairs := 40
+			renLimit := 10
+
+			renLedger := newMockRenLedger()
+
+			// Initial sync with an empty Ren Ledger should return an empty changeset
+			syncer := NewSyncer(&renLedger, renLimit)
+			changeSet, err := syncer.Sync()
+			Expect(err).ShouldNot(HaveOccurred())
+			Expect(len(changeSet)).Should(Equal(0))
+
+			// Opening some buy and sell orders on the Ren Ledger
+			err = renLedger.openBuyAndSellOrders(numberOfOrderPairs)
+			Expect(err).ShouldNot(HaveOccurred())
+
+			// Sync and expect 2*renLimit number of change sets to be created
+			changeSet, err = syncer.Sync()
+			Expect(err).ShouldNot(HaveOccurred())
+			Expect(len(changeSet)).Should(Equal(renLimit * 2))
+
+			// Randomly cancel a quarter of the orders
+			for i := 0; i < numberOfOrderPairs/4; i++ {
+				id := generateRandomOrderID(2 * numberOfOrderPairs)
+				err = renLedger.CancelOrder([65]byte{}, [32]byte{byte(id)})
+			}
+			// Sync and expect canceled orders to be purged
+			changeSet, err = syncer.Sync()
+			Expect(err).ShouldNot(HaveOccurred())
+			Expect(len(changeSet) < renLimit*4).Should(BeTrue())
+
+			// Randomly confirm half of the total orders
+			for i := 0; i < numberOfOrderPairs/2; i++ {
+				id := generateRandomOrderID(2 * numberOfOrderPairs)
+				matchID := [32]byte{byte(id + 1)}
+				err = renLedger.ConfirmOrder([32]byte{byte(id)}, matchID)
+			}
+
+			// Sync and expect confirmed orders to have purged
+			changeSet, err = syncer.Sync()
+			Expect(err).ShouldNot(HaveOccurred())
+			Expect(len(changeSet) < renLimit*8).Should(BeTrue())
+		})
+	})
+})
+
+type mockOrder struct {
+	status   order.Status
+	parity   order.Parity
+	priority uint64
+}
+
+type mockRenLedger struct {
+	buyOrdersMu *sync.Mutex
+	buyOrders   []order.ID
+
+	sellOrdersMu *sync.Mutex
+	sellOrders   []order.ID
+
+	ordersMu *sync.Mutex
+	orders   map[order.ID]mockOrder
+}
+
+func newMockRenLedger() mockRenLedger {
+	return mockRenLedger{
+		buyOrdersMu: new(sync.Mutex),
+		buyOrders:   []order.ID{},
+
+		sellOrdersMu: new(sync.Mutex),
+		sellOrders:   []order.ID{},
+
+		ordersMu: new(sync.Mutex),
+		orders:   map[order.ID]mockOrder{},
+	}
+}
+
+func (renLedger *mockRenLedger) OpenBuyOrder(signature [65]byte, orderID order.ID) error {
+	renLedger.ordersMu.Lock()
+	renLedger.buyOrdersMu.Lock()
+	defer renLedger.ordersMu.Unlock()
+	defer renLedger.buyOrdersMu.Unlock()
+
+	if _, ok := renLedger.orders[orderID]; !ok {
+		renLedger.orders[orderID] = mockOrder{
+			status:   order.Open,
+			parity:   order.ParityBuy,
+			priority: binary.LittleEndian.Uint64(orderID[:]),
+		}
+		renLedger.buyOrders = append(renLedger.buyOrders, orderID)
+		return nil
+	}
+	return errors.New("cannot open order that is already open")
+}
+
+func (renLedger *mockRenLedger) OpenSellOrder(signature [65]byte, orderID order.ID) error {
+	renLedger.ordersMu.Lock()
+	renLedger.sellOrdersMu.Lock()
+	defer renLedger.ordersMu.Unlock()
+	defer renLedger.sellOrdersMu.Unlock()
+
+	if _, ok := renLedger.orders[orderID]; !ok {
+		renLedger.orders[orderID] = mockOrder{
+			status:   order.Open,
+			parity:   order.ParitySell,
+			priority: binary.LittleEndian.Uint64(orderID[:]),
+		}
+		renLedger.sellOrders = append(renLedger.sellOrders, orderID)
+		return nil
+	}
+	return errors.New("cannot open order that is already open")
+}
+
+func (renLedger *mockRenLedger) CancelOrder(signature [65]byte, orderID order.ID) error {
+	return renLedger.setOrderStatus(orderID, order.Canceled)
+}
+
+func (renLedger *mockRenLedger) ConfirmOrder(id order.ID, match order.ID) error {
+	if err := renLedger.setOrderStatus(id, order.Confirmed); err != nil {
+		return fmt.Errorf("cannot confirm order that is not open: %v", err)
+	}
+	renLedger.setOrderStatus(match, order.Confirmed)
+	return nil
+}
+
+func (renLedger *mockRenLedger) Fee() (*big.Int, error) {
+	return big.NewInt(0), nil
+}
+
+func (renLedger *mockRenLedger) Status(orderID order.ID) (order.Status, error) {
+	renLedger.ordersMu.Lock()
+	defer renLedger.ordersMu.Unlock()
+
+	if ord, ok := renLedger.orders[orderID]; ok {
+		return ord.status, nil
+	}
+	return order.Nil, ErrCannotFindOrder
+}
+
+func (renLedger *mockRenLedger) Priority(orderID order.ID) (uint64, error) {
+	renLedger.ordersMu.Lock()
+	defer renLedger.ordersMu.Unlock()
+
+	if ord, ok := renLedger.orders[orderID]; ok {
+		return ord.priority, nil
+	}
+	return uint64(0), ErrCannotFindOrder
+}
+
+func (renLedger *mockRenLedger) Trader(orderID order.ID) (string, error) {
+	panic("unimplemented")
+}
+
+func (renLedger *mockRenLedger) OrderMatch(orderID order.ID) (order.ID, error) {
+	panic("unimplemented")
+}
+
+func (renLedger *mockRenLedger) Depth(orderID order.ID) (uint, error) {
+	panic("unimplemented")
+}
+
+func (renLedger *mockRenLedger) BuyOrders(offset, limit int) ([]order.ID, error) {
+	renLedger.ordersMu.Lock()
+	renLedger.buyOrdersMu.Lock()
+	defer renLedger.ordersMu.Unlock()
+	defer renLedger.buyOrdersMu.Unlock()
+
+	orders := []order.ID{}
+	end := offset + limit
+	if end > len(renLedger.buyOrders) {
+		end = len(renLedger.buyOrders)
+	}
+	for i := offset; i < end; i++ {
+		orderID := renLedger.buyOrders[i]
+		if buyOrder, ok := renLedger.orders[orderID]; ok {
+			if buyOrder.parity == order.ParityBuy && buyOrder.status == order.Open {
+				orders = append(orders, orderID)
+			}
+		}
+
+	}
+	return orders, nil
+}
+
+func (renLedger *mockRenLedger) SellOrders(offset, limit int) ([]order.ID, error) {
+	renLedger.ordersMu.Lock()
+	renLedger.buyOrdersMu.Lock()
+	defer renLedger.ordersMu.Unlock()
+	defer renLedger.buyOrdersMu.Unlock()
+
+	orders := []order.ID{}
+	end := offset + limit
+	if end > len(renLedger.sellOrders) {
+		end = len(renLedger.sellOrders)
+	}
+	for i := offset; i < end; i++ {
+		orderID := renLedger.sellOrders[i]
+		if sellOrder, ok := renLedger.orders[orderID]; ok {
+			if sellOrder.parity == order.ParitySell && sellOrder.status == order.Open {
+				orders = append(orders, orderID)
+			}
+		}
+	}
+	return orders, nil
+}
+
+func (renLedger *mockRenLedger) setOrderStatus(orderID order.ID, status order.Status) error {
+	renLedger.ordersMu.Lock()
+	defer renLedger.ordersMu.Unlock()
+
+	if _, ok := renLedger.orders[orderID]; ok {
+		ord := renLedger.orders[orderID]
+		ord.status = status
+		renLedger.orders[orderID] = ord
+		return nil
+	}
+	return ErrCannotFindOrder
+}
+
+func (renLedger *mockRenLedger) openBuyAndSellOrders(n int) error {
+	for i := 0; i < 2*n; i += 2 {
+		if err := renLedger.OpenBuyOrder([65]byte{}, [32]byte{byte(i)}); err != nil {
+			return err
+		}
+		if err := renLedger.OpenSellOrder([65]byte{}, [32]byte{byte(i + 1)}); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+func generateRandomOrderID(numberOfOrders int) int {
+	rand.Seed(time.Now().UTC().UnixNano())
+	return rand.Intn(numberOfOrders)
+}
diff --git a/shamir/shamir.go b/shamir/shamir.go
index aff910e5..9040c486 100644
--- a/shamir/shamir.go
+++ b/shamir/shamir.go
@@ -71,8 +71,12 @@ func (share *Share) Equal(other *Share) bool {
 // using binary.BigEndian.
 func (share Share) MarshalBinary() ([]byte, error) {
 	buf := new(bytes.Buffer)
-	binary.Write(buf, binary.BigEndian, share.Index)
-	binary.Write(buf, binary.BigEndian, share.Value)
+	if err := binary.Write(buf, binary.BigEndian, share.Index); err != nil {
+		return nil, err
+	}
+	if err := binary.Write(buf, binary.BigEndian, share.Value); err != nil {
+		return nil, err
+	}
 	return buf.Bytes(), nil
 }
 
@@ -84,8 +88,12 @@ func (share *Share) UnmarshalBinary(data []byte) error {
 		return ErrUnmarshalNilBytes
 	}
 	buf := bytes.NewBuffer(data)
-	binary.Read(buf, binary.BigEndian, &share.Index)
-	binary.Read(buf, binary.BigEndian, &share.Value)
+	if err := binary.Read(buf, binary.BigEndian, &share.Index); err != nil {
+		return err
+	}
+	if err := binary.Read(buf, binary.BigEndian, &share.Value); err != nil {
+		return err
+	}
 	return nil
 }
 
diff --git a/smpc/builder.go b/smpc/builder.go
index 640eed7b..0e0726bd 100644
--- a/smpc/builder.go
+++ b/smpc/builder.go
@@ -36,6 +36,21 @@ func NewShareBuilder(k int64) *ShareBuilder {
 	}
 }
 
+func (builder *ShareBuilder) Shares(id [32]byte, buffer []shamir.Share) int {
+	builder.sharesMu.Lock()
+	defer builder.sharesMu.Unlock()
+
+	i := int64(0)
+	for _, share := range builder.shares[id] {
+		if i >= builder.k || i >= int64(len(buffer)) {
+			break
+		}
+		buffer[i] = share
+		i++
+	}
+	return int(i)
+}
+
 func (builder *ShareBuilder) Insert(id [32]byte, share shamir.Share) error {
 	builder.sharesMu.Lock()
 	defer builder.sharesMu.Unlock()
@@ -84,7 +99,6 @@ func (builder *ShareBuilder) Observe(id, networkID [32]byte, observer ShareBuild
 }
 
 func (builder *ShareBuilder) notify(id [32]byte, val uint64) {
-	log.Println("notifiying!")
 	if observers, ok := builder.observers[id]; ok {
 		for networkID, observer := range observers {
 			observer.OnNotifyBuild(id, networkID, val)
@@ -93,8 +107,7 @@ func (builder *ShareBuilder) notify(id [32]byte, val uint64) {
 }
 
 func (builder *ShareBuilder) tryJoin(id [32]byte) (uint64, error) {
-	log.Println("joining value", base64.StdEncoding.EncodeToString(id[:]), "with len", len(builder.shares[id]), "vs", builder.k)
-
+	log.Printf("[join => %v] value = %v", len(builder.shares[id]), base64.StdEncoding.EncodeToString(id[:8]))
 	if int64(len(builder.shares[id])) >= builder.k {
 		builder.sharesCache = builder.sharesCache[0:0]
 		k := int64(0)
diff --git a/smpc/smpc.go b/smpc/smpc.go
index 20c79832..b4d74b2f 100644
--- a/smpc/smpc.go
+++ b/smpc/smpc.go
@@ -9,6 +9,7 @@ import (
 
 	"github.com/republicprotocol/republic-go/dispatch"
 	"github.com/republicprotocol/republic-go/identity"
+	"github.com/republicprotocol/republic-go/shamir"
 	"github.com/republicprotocol/republic-go/stream"
 	"github.com/republicprotocol/republic-go/swarm"
 	"golang.org/x/net/context"
@@ -68,8 +69,9 @@ type smpcer struct {
 	lookupMu *sync.RWMutex
 	lookup   map[identity.Address]identity.MultiAddress
 
-	shareBuildersMu *sync.RWMutex
-	shareBuilders   map[[32]byte]*ShareBuilder
+	shareBuildersMu       *sync.RWMutex
+	shareBuilders         map[[32]byte]*ShareBuilder
+	shareBuildersJoinable map[[32]byte]bool
 
 	ctxCancelsMu *sync.Mutex
 	ctxCancels   map[[32]byte]map[identity.Address]context.CancelFunc
@@ -95,8 +97,9 @@ func NewSmpcer(swarmer swarm.Swarmer, streamer stream.Streamer, buffer int) Smpc
 		lookupMu: new(sync.RWMutex),
 		lookup:   map[identity.Address]identity.MultiAddress{},
 
-		shareBuildersMu: new(sync.RWMutex),
-		shareBuilders:   map[[32]byte]*ShareBuilder{},
+		shareBuildersMu:       new(sync.RWMutex),
+		shareBuilders:         map[[32]byte]*ShareBuilder{},
+		shareBuildersJoinable: map[[32]byte]bool{},
 
 		ctxCancelsMu: new(sync.Mutex),
 		ctxCancels:   map[[32]byte]map[identity.Address]context.CancelFunc{},
@@ -157,7 +160,6 @@ func (smpc *smpcer) Results() <-chan Result {
 // notify the Smpcer that a value of interest has been reconstructed by the
 // ShareBuilder.
 func (smpc *smpcer) OnNotifyBuild(id, networkID [32]byte, value uint64) {
-	log.Println("notified successfully!")
 	result := Result{
 		InstID:    id,
 		NetworkID: networkID,
@@ -275,7 +277,7 @@ func (smpc *smpcer) instJ(instID, networkID [32]byte, inst InstJ) {
 	if shareBuilder, ok := smpc.shareBuilders[networkID]; ok {
 		shareBuilder.Observe(instID, networkID, smpc)
 	}
-	smpc.processMessageJ(*msg.MessageJ)
+	smpc.processMessageJ(nil, *msg.MessageJ)
 
 	for _, addr := range smpc.network[networkID] {
 		go smpc.sendMessage(addr, &msg)
@@ -319,25 +321,46 @@ func (smpc *smpcer) stream(remoteAddr identity.Address, remoteStream stream.Stre
 
 		switch msg.MessageType {
 		case MessageTypeJ:
-			smpc.processMessageJ(*msg.MessageJ)
+			smpc.processMessageJ(&remoteAddr, *msg.MessageJ)
 		default:
 			log.Printf("cannot recv message from %v: %v", remoteAddr, ErrUnexpectedMessageType)
 		}
 	}
 }
 
-func (smpc *smpcer) processMessageJ(message MessageJ) {
+func (smpc *smpcer) processMessageJ(remoteAddr *identity.Address, message MessageJ) {
 	smpc.shareBuildersMu.RLock()
 	defer smpc.shareBuildersMu.RUnlock()
 
+	if remoteAddr == nil {
+		// we sent this message to ourselves
+		smpc.shareBuildersJoinable[message.InstID] = true
+	}
+
 	if shareBuilder, ok := smpc.shareBuilders[message.NetworkID]; ok {
-		log.Println("inserting value into share builder")
+		if remoteAddr != nil {
+			go func() {
+				shares := [16]shamir.Share{}
+				n := shareBuilder.Shares(message.InstID, shares[:])
+
+				for i := 0; i < n; i++ {
+					msg := Message{
+						MessageType: MessageTypeJ,
+						MessageJ: &MessageJ{
+							InstID:    message.InstID,
+							NetworkID: message.NetworkID,
+							Share:     shares[i],
+						},
+					}
+					smpc.sendMessage(*remoteAddr, &msg)
+				}
+			}()
+		}
 		if err := shareBuilder.Insert(message.InstID, message.Share); err != nil {
-			if err == ErrInsufficientSharesToJoin {
+			if err != ErrInsufficientSharesToJoin {
+				log.Printf("could not insert share: %v", err)
 				return
 			}
-			log.Printf("could not insert share: %v", err)
-			return
 		}
 	}
 }
diff --git a/smpc/smpc_test.go b/smpc/smpc_test.go
index daddf7dd..1c897891 100644
--- a/smpc/smpc_test.go
+++ b/smpc/smpc_test.go
@@ -19,15 +19,10 @@ import (
 
 var _ = Describe("Smpc", func() {
 
-	var hub stream.ChannelHub
-
-	BeforeEach(func() {
-		hub = stream.NewChannelHub()
-	})
-
 	Context("when starting", func() {
 
 		It("should return error if smpcer has already been started", func() {
+			hub := stream.NewChannelHub()
 			smpcer, _, err := createSMPCer(&hub)
 			Expect(err).ShouldNot(HaveOccurred())
 			err = smpcer.Start()
@@ -43,6 +38,7 @@ var _ = Describe("Smpc", func() {
 	Context("when shutting down", func() {
 
 		It("should return error if smpcer is not running", func() {
+			hub := stream.NewChannelHub()
 			smpcer, _, err := createSMPCer(&hub)
 			Expect(err).ShouldNot(HaveOccurred())
 
@@ -52,6 +48,7 @@ var _ = Describe("Smpc", func() {
 		})
 
 		It("should not return error if smpcer is running", func() {
+			hub := stream.NewChannelHub()
 			smpcer, _, err := createSMPCer(&hub)
 			Expect(err).ShouldNot(HaveOccurred())
 			err = smpcer.Start()
@@ -66,6 +63,7 @@ var _ = Describe("Smpc", func() {
 
 		It("should join shares to obtain final values", func() {
 			// Create 16 smpcers and issue 5 random secrets
+			hub := stream.NewChannelHub()
 			count, err := runSmpcers(24, 5, 0, &hub)
 			Expect(err).ShouldNot(HaveOccurred())
 			Expect(count).To(Equal(5))
@@ -75,20 +73,22 @@ var _ = Describe("Smpc", func() {
 		It("should join shares when faults are below the threshold", func() {
 			// Create 24 smpcers and issue 5 random secrets
 			// Do not start 1/3 of the smpcers (for example: 7)
-			count, err := runSmpcers(24, 5, 7, &hub)
+			hub := stream.NewChannelHub()
+			count, err := runSmpcers(24, 5, 8, &hub)
 			Expect(err).ShouldNot(HaveOccurred())
 			Expect(count).To(Equal(5))
 		})
 
 		It("should not join shares when faults are above the threshold", func() {
+			hub := stream.NewChannelHub()
 			timer := time.NewTimer(time.Second * 4)
 			count := int32(0)
 
 			go func() {
 				defer GinkgoRecover()
 
-				// Create 24 smpcers and do not start 10 smpcers
-				c, err := runSmpcers(24, 5, 10, &hub)
+				// Create 24 smpcers and do not start 16 smpcers
+				c, err := runSmpcers(24, 5, 16, &hub)
 				Expect(err).ShouldNot(HaveOccurred())
 				atomic.StoreInt32(&count, int32(c))
 			}()
@@ -101,6 +101,7 @@ var _ = Describe("Smpc", func() {
 		It("should join when nodes are in multiple non-overlapping networks", func() {
 			// Run 12 smpcers in 2 networks such that each network has 6 smpcers which
 			// in such a way the that each network has seperate smpcers.
+			hub := stream.NewChannelHub()
 			count, err := runSmpcersInTwoNetworks(12, 6, &hub)
 			Expect(err).ShouldNot(HaveOccurred())
 			Expect(count).To(Equal(2))
@@ -109,6 +110,7 @@ var _ = Describe("Smpc", func() {
 		It("should join when nodes are in multiple overlapping networks", func() {
 			// Run 9 smpcers in 2 networks such that each network has 6 smpcers
 			// (with replacement) and issue unique random secrets to each of the networks
+			hub := stream.NewChannelHub()
 			count, err := runSmpcersInTwoNetworks(9, 6, &hub)
 			Expect(err).ShouldNot(HaveOccurred())
 			Expect(count).To(Equal(2))
@@ -149,7 +151,7 @@ func createSMPCer(hub *stream.ChannelHub) (Smpcer, identity.Address, error) {
 
 func runSmpcers(numberOfSmpcers, numberOfJoins, numberOfDeadNodes int, hub *stream.ChannelHub) (int, error) {
 
-	smpcers, addrs, err := createAddressesAndStartSmpcers(numberOfSmpcers, numberOfDeadNodes, hub)
+	smpcers, addrs, err := createAddressesAndStartSmpcers(numberOfSmpcers, hub)
 	if err != nil {
 		return 0, err
 	}
@@ -186,7 +188,7 @@ func runSmpcers(numberOfSmpcers, numberOfJoins, numberOfDeadNodes int, hub *stre
 // Runs smpcers in two networks (either overlapped or not) and sends different secrets to both
 func runSmpcersInTwoNetworks(numberOfSmpcers, minimumNumberOfSmpcers int, hub *stream.ChannelHub) (int, error) {
 
-	smpcers, addrs, err := createAddressesAndStartSmpcers(numberOfSmpcers, 0, hub)
+	smpcers, addrs, err := createAddressesAndStartSmpcers(numberOfSmpcers, hub)
 	if err != nil {
 		return 0, err
 	}
@@ -244,13 +246,13 @@ func runSmpcersInTwoNetworks(numberOfSmpcers, minimumNumberOfSmpcers int, hub *s
 	return count, nil
 }
 
-func createAddressesAndStartSmpcers(numberOfSmpcers, numberOfDeadNodes int, hub *stream.ChannelHub) (map[int]Smpcer, identity.Addresses, error) {
+func createAddressesAndStartSmpcers(numberOfSmpcers int, hub *stream.ChannelHub) (map[int]Smpcer, identity.Addresses, error) {
 	var err error
 	smpcers := make(map[int]Smpcer, numberOfSmpcers)
 	addrs := make(identity.Addresses, numberOfSmpcers)
 
 	// Generate Smpcers with addresses and start them
-	for i := numberOfDeadNodes; i < numberOfSmpcers; i++ {
+	for i := 0; i < numberOfSmpcers; i++ {
 		smpcers[i], addrs[i], err = createSMPCer(hub)
 		if err != nil {
 			return smpcers, addrs, err
diff --git a/stream/channel.go b/stream/channel.go
index 383bf0d4..5f864050 100644
--- a/stream/channel.go
+++ b/stream/channel.go
@@ -17,24 +17,39 @@ var ErrAlreadyRegistered = errors.New("client has already been registered with t
 // channelStream must not be used unless it was returned from a call to
 // channelStreamer.Open.
 type channelStream struct {
-	send chan []byte
-	recv chan []byte
+	sendMu *sync.RWMutex
+	send   chan []byte
+	recvMu *sync.RWMutex
+	recv   chan []byte
+	closed *bool
 }
 
 // Send implements the Stream interface by marshaling the Message to binary and
 // writing it to the sending channel.
 func (stream channelStream) Send(message Message) error {
+	stream.sendMu.RLock()
+	defer stream.sendMu.RUnlock()
+
+	if *stream.closed {
+		return ErrSendOnClosedStream
+	}
 	data, err := message.MarshalBinary()
 	if err != nil {
 		return err
 	}
 	stream.send <- data
-	return nil
+	return err
 }
 
 // Recv implements the Stream interface by reading from the receiving channel
 // and unmarshaling the data into a Message.
 func (stream channelStream) Recv(message Message) error {
+	stream.recvMu.RLock()
+	defer stream.recvMu.RUnlock()
+
+	if *stream.closed {
+		return ErrRecvOnClosedStream
+	}
 	data, ok := <-stream.recv
 	if !ok {
 		return ErrRecvOnClosedStream
@@ -42,6 +57,21 @@ func (stream channelStream) Recv(message Message) error {
 	return message.UnmarshalBinary(data)
 }
 
+func (stream channelStream) Close() {
+	stream.sendMu.Lock()
+	stream.recvMu.Lock()
+	defer stream.sendMu.Unlock()
+	defer stream.recvMu.Unlock()
+
+	if *stream.closed {
+		return
+	}
+
+	close(stream.send)
+	close(stream.recv)
+	*stream.closed = true
+}
+
 // A ChannelHub will store a map of all active channelStreams between
 // identity.Addresses and ensures that the mapping is symmetrical.
 type ChannelHub struct {
@@ -57,7 +87,7 @@ func NewChannelHub() ChannelHub {
 	}
 }
 
-func (hub *ChannelHub) register(clientAddr, serverAddr identity.Address) Stream {
+func (hub *ChannelHub) register(clientAddr, serverAddr identity.Address) channelStream {
 	hub.connsMu.Lock()
 	defer hub.connsMu.Unlock()
 
@@ -73,12 +103,9 @@ func (hub *ChannelHub) register(clientAddr, serverAddr identity.Address) Stream
 	// and panic for clarity and easier debugging
 	_, clientOk := hub.conns[clientAddr][serverAddr]
 	_, serverOk := hub.conns[serverAddr][clientAddr]
-	if clientOk && !serverOk {
+	if (clientOk && !serverOk) || serverOk && !clientOk {
 		panic("assymetric connection from client to server")
 	}
-	if serverOk && !clientOk {
-		panic("assymetric connection from server to client")
-	}
 
 	// A symmetric connection has already been established
 	if clientOk && serverOk {
@@ -86,13 +113,20 @@ func (hub *ChannelHub) register(clientAddr, serverAddr identity.Address) Stream
 	}
 
 	// Build a symmetric connection between the client and the server
+	closed := false
 	hub.conns[clientAddr][serverAddr] = channelStream{
-		send: make(chan []byte),
-		recv: make(chan []byte),
+		sendMu: new(sync.RWMutex),
+		send:   make(chan []byte),
+		recvMu: new(sync.RWMutex),
+		recv:   make(chan []byte),
+		closed: &closed,
 	}
 	hub.conns[serverAddr][clientAddr] = channelStream{
-		send: hub.conns[clientAddr][serverAddr].recv,
-		recv: hub.conns[clientAddr][serverAddr].send,
+		sendMu: hub.conns[clientAddr][serverAddr].recvMu,
+		send:   hub.conns[clientAddr][serverAddr].recv,
+		recvMu: hub.conns[clientAddr][serverAddr].sendMu,
+		recv:   hub.conns[clientAddr][serverAddr].send,
+		closed: &closed,
 	}
 	return hub.conns[clientAddr][serverAddr]
 }
@@ -116,5 +150,10 @@ func NewChannelStreamer(addr identity.Address, hub *ChannelHub) Streamer {
 // Open implements the Streamer interface by using the ChannelHub to register
 // connections between two identity.Addresses.
 func (streamer *channelStreamer) Open(ctx context.Context, multiAddr identity.MultiAddress) (Stream, error) {
-	return streamer.hub.register(streamer.addr, multiAddr.Address()), nil
+	stream := streamer.hub.register(streamer.addr, multiAddr.Address())
+	go func() {
+		<-ctx.Done()
+		stream.Close()
+	}()
+	return stream, nil
 }
diff --git a/stream/channel_test.go b/stream/channel_test.go
index 7e237a43..a248cbd5 100644
--- a/stream/channel_test.go
+++ b/stream/channel_test.go
@@ -2,6 +2,7 @@ package stream_test
 
 import (
 	"context"
+	"time"
 
 	. "github.com/onsi/ginkgo"
 	. "github.com/onsi/gomega"
@@ -97,6 +98,22 @@ var _ = Describe("Channel streams", func() {
 			})
 		})
 
+		It("should return an error when sending on a closed stream", func() {
+			clientCancel()
+			time.Sleep(time.Second)
+			message := mockMessage([]byte{0})
+			err := clientStream.Send(&message)
+			Expect(err).Should(Equal(ErrSendOnClosedStream))
+		})
+
+		It("should return an error when receiving on a closed stream", func() {
+			clientCancel()
+			time.Sleep(time.Second)
+			message := mockMessage{}
+			err := clientStream.Recv(&message)
+			Expect(err).Should(Equal(ErrRecvOnClosedStream))
+		})
+
 	})
 })
 
diff --git a/stream/stream.go b/stream/stream.go
index 8c9a9c9d..7e7e2a48 100644
--- a/stream/stream.go
+++ b/stream/stream.go
@@ -4,7 +4,6 @@ import (
 	"context"
 	"encoding"
 	"errors"
-	"log"
 	"sync"
 
 	"github.com/republicprotocol/republic-go/identity"
@@ -95,10 +94,8 @@ func NewStreamer(addr identity.Address, client Client, server Server) Streamer {
 func (streamer streamer) Open(ctx context.Context, multiAddr identity.MultiAddress) (Stream, error) {
 	addr := multiAddr.Address()
 	if streamer.addr < addr {
-		log.Println("client connecting to server")
 		return streamer.client.Connect(ctx, multiAddr)
 	}
-	log.Println("server listening to client")
 	return streamer.server.Listen(ctx, addr)
 }
 
diff --git a/stream/stream_test.go b/stream/stream_test.go
index 5a9f8e95..82b17716 100644
--- a/stream/stream_test.go
+++ b/stream/stream_test.go
@@ -14,7 +14,7 @@ import (
 	"github.com/republicprotocol/republic-go/identity"
 )
 
-const numberOfNodes = 128
+const numberOfNodes = 32
 
 var _ = Describe("Streaming", func() {
 
@@ -137,6 +137,8 @@ var _ = Describe("Streaming", func() {
 
 			// Cancel all but the last connection
 			for i := 0; i < numberOfNodes; i++ {
+				clients[i].streamsMu.Lock()
+				servers[i].streamsMu.Lock()
 				for j := 0; j < numberOfNodes; j++ {
 					if i == j {
 						continue
@@ -146,6 +148,8 @@ var _ = Describe("Streaming", func() {
 						Expect(len(clients[i].streams) + len(servers[i].streams)).Should(Equal(numberOfNodes - 1))
 					}
 				}
+				clients[i].streamsMu.Unlock()
+				servers[i].streamsMu.Unlock()
 			}
 
 			// Cancel the last connection
@@ -159,14 +163,18 @@ var _ = Describe("Streaming", func() {
 			}
 
 			// Expect shutdown of streams
-			time.Sleep(time.Millisecond)
+			time.Sleep(time.Second)
 			for i := 0; i < numberOfNodes; i++ {
+				clients[i].streamsMu.Lock()
+				servers[i].streamsMu.Lock()
 				for j := 0; j < numberOfNodes; j++ {
 					if i == j {
 						continue
 					}
 					Expect(len(clients[i].streams) + len(servers[i].streams)).Should(Equal(0))
 				}
+				clients[i].streamsMu.Unlock()
+				servers[i].streamsMu.Unlock()
 			}
 		})
 	})
diff --git a/swarm/swarm.go b/swarm/swarm.go
index 88ecc3aa..8bdbdb16 100644
--- a/swarm/swarm.go
+++ b/swarm/swarm.go
@@ -183,6 +183,12 @@ func (swarmer *swarmer) query(ctx context.Context, query identity.Address, depth
 		ctx, cancel := context.WithTimeout(context.Background(), time.Minute)
 		defer cancel()
 
+		if isBootstrapping {
+			if _, err := swarmer.client.Ping(ctx, peer); err != nil {
+				continue
+			}
+		}
+
 		// Query for identity.MultiAddresses that are closer to the query
 		// target than the peer itself, and add them to the whitelist
 		multiAddrs, err := swarmer.client.Query(ctx, peer, query, [65]byte{})
diff --git a/swarm/swarm_test.go b/swarm/swarm_test.go
index a37b364d..78ac916f 100644
--- a/swarm/swarm_test.go
+++ b/swarm/swarm_test.go
@@ -2,10 +2,12 @@ package swarm_test
 
 import (
 	"context"
+	"log"
 	"sync"
 
 	. "github.com/onsi/ginkgo"
 	. "github.com/onsi/gomega"
+	"github.com/republicprotocol/republic-go/dispatch"
 	. "github.com/republicprotocol/republic-go/swarm"
 
 	"github.com/republicprotocol/republic-go/crypto"
@@ -15,120 +17,70 @@ import (
 
 var _ = Describe("Swarm", func() {
 
-	Context("when using a fully connected swarmer", func() {
+	Context("when bootstrapping", func() {
 
-		It("should not error on bootstrap or query", func() {
-			var err error
+		It("should be able to query any peer after bootstrapping", func() {
 			numberOfClients := 50
+			numberOfBootstrapClients := 5
 
 			// Creating clients
+			dhts := make([]dht.DHT, numberOfClients)
 			clients := make([]Client, numberOfClients)
-			multiaddrs := make(identity.MultiAddresses, numberOfClients)
+			multiAddrs := make(identity.MultiAddresses, numberOfClients)
+			bootstrapMultiaddrs := make(identity.MultiAddresses, numberOfBootstrapClients)
+
+			// Creating a common server hub for all clients to use
+			serverHub := &mockServerHub{
+				connsMu: new(sync.Mutex),
+				conns:   map[identity.Address]Server{},
+			}
+
 			for i := 0; i < numberOfClients; i++ {
-				clients[i], err = newMockClient(multiaddrs)
+				client, err := newMockClientToServer(serverHub)
 				Expect(err).ShouldNot(HaveOccurred())
-				multiaddrs[i] = clients[i].MultiAddress()
+				clients[i] = &client
+				multiAddrs[i] = clients[i].MultiAddress()
+
+				// Store bootstrap multiAddresses
+				if i < numberOfBootstrapClients {
+					bootstrapMultiaddrs[i] = multiAddrs[i]
+				}
+
+				dhts[i] = dht.NewDHT(multiAddrs[i].Address(), numberOfClients)
+				server := NewServer(clients[i], &dhts[i])
+				serverHub.Register(multiAddrs[i].Address(), server)
 			}
 
-			var swarmer Swarmer
 			ctx, cancelCtx := context.WithCancel(context.Background())
 			defer cancelCtx()
 
 			// Bootstrapping created clients
-			for i := 0; i < numberOfClients; i++ {
-				// Creating swarmer for the client
-				clientDht := dht.NewDHT(multiaddrs[i].Address(), 100)
-				swarmer = NewSwarmer(clients[i], &clientDht)
+			dispatch.CoForAll(numberOfClients, func(i int) {
+				defer GinkgoRecover()
 
-				// Bootstrap the client with all available multiaddresses
-				err = swarmer.Bootstrap(ctx, multiaddrs)
+				// Creating swarmer for the client
+				swarmer := NewSwarmer(clients[i], &dhts[i])
+				err := swarmer.Bootstrap(ctx, bootstrapMultiaddrs)
 				Expect(err).ShouldNot(HaveOccurred())
-				Expect(len(clientDht.MultiAddresses())).To(Equal(numberOfClients - 1))
-			}
-
-			// Query for the last created client
-			multiaddr, err := swarmer.Query(ctx, multiaddrs[0].Address(), 1)
-			Expect(err).ShouldNot(HaveOccurred())
-			Expect(multiaddr).To(Equal(multiaddrs[0]))
+			})
 
-			// Create a new client
-			newClient, err := newMockClient(multiaddrs)
-			Expect(err).ShouldNot(HaveOccurred())
-
-			// Creating swarmer for the newly created client
-			newDht := dht.NewDHT(newClient.MultiAddress().Address(), 100)
-			newSwarmer := NewSwarmer(newClient, &newDht)
+			for i := 0; i < numberOfClients; i++ {
+				log.Println(len(dhts[i].MultiAddresses()))
+			}
 
-			// Bootstrapping by providing only the first client address should
-			// populate the new client's DHT with all registered client multiaddresses
-			err = newSwarmer.Bootstrap(ctx, identity.MultiAddresses{multiaddrs[0]})
-			Expect(err).ShouldNot(HaveOccurred())
-			Expect(len(newDht.MultiAddresses())).To(Equal(numberOfClients))
+			// Query for clients
+			for i := 0; i < numberOfClients; i++ {
+				for j := 0; j < numberOfClients; j++ {
+					if i == j {
+						continue
+					}
+					multiAddr, err := NewSwarmer(clients[i], &dhts[i]).Query(ctx, multiAddrs[j].Address(), -1)
+					Expect(err).ShouldNot(HaveOccurred())
+					Expect(multiAddr).To(Equal(multiAddrs[j]))
+				}
+			}
 		})
 	})
-
-	// FIXME: The functionality works everywhere it is used but this test does not.
-	// This test must be fixed and re-enabled.
-
-	//Context("when clients are connected to servers for querying", func() {
-	//
-	//	It("should connect all clients using swarmer", func() {
-	//		numberOfClients := 50
-	//		numberOfBootstrapClients := 5
-	//
-	//		// Creating clients
-	//		clients := make([]Client, numberOfClients)
-	//		multiaddrs := make(identity.MultiAddresses, numberOfClients)
-	//		dhts := make([]dht.DHT, numberOfClients)
-	//		bootstrapMultiaddrs := make(identity.MultiAddresses, numberOfBootstrapClients)
-	//
-	//		// Creating a common server hub for all clients to use
-	//		serverHub := &mockServerHub{
-	//			connsMu: new(sync.Mutex),
-	//			conns:   map[identity.Address]Server{},
-	//		}
-	//
-	//		for i := 0; i < numberOfClients; i++ {
-	//			client, err := newMockClientToServer(serverHub)
-	//			Expect(err).ShouldNot(HaveOccurred())
-	//			clients[i] = &client
-	//			multiaddrs[i] = clients[i].MultiAddress()
-	//
-	//			// Store bootstrap multiaddresses
-	//			if i < numberOfBootstrapClients {
-	//				bootstrapMultiaddrs[i] = multiaddrs[i]
-	//			}
-	//
-	//			// TODO: (Please confirm) Creating a server for each client (??)
-	//			dhts[i] = dht.NewDHT(multiaddrs[i].Address(), 100)
-	//			server := NewServer(clients[i], &dhts[i])
-	//			serverHub.Register(multiaddrs[i].Address(), server)
-	//		}
-	//
-	//		var swarmer Swarmer
-	//		ctx, cancelCtx := context.WithCancel(context.Background())
-	//		defer cancelCtx()
-	//
-	//		// Bootstrapping created clients
-	//		for i := 0; i < numberOfClients; i++ {
-	//			// Creating swarmer for the client
-	//			swarmer = NewSwarmer(clients[i], &dhts[i])
-	//
-	//			err := swarmer.Bootstrap(ctx, bootstrapMultiaddrs)
-	//			Expect(err).ShouldNot(HaveOccurred())
-	//
-	//			log.Println(len(dhts[i].MultiAddresses()))
-	//			if i > numberOfBootstrapClients {
-	//				Expect(len(dhts[i].MultiAddresses()) > numberOfBootstrapClients).To(Equal(true))
-	//			}
-	//		}
-	//
-	//		// Query for the last created client
-	//		multiaddr, err := swarmer.Query(ctx, multiaddrs[0].Address(), 1)
-	//		Expect(err).ShouldNot(HaveOccurred())
-	//		Expect(multiaddr).To(Equal(multiaddrs[0]))
-	//	})
-	//})
 })
 
 // mockServerHub will store all Servers that Clients use to Query and Ping
@@ -148,12 +100,24 @@ func (serverHub *mockServerHub) Register(serverAddr identity.Address, server Ser
 }
 
 type mockClientToServer struct {
-	multiaddr identity.MultiAddress
+	multiAddr identity.MultiAddress
 	serverHub *mockServerHub
 }
 
+func newMockClientToServer(mockServerHub *mockServerHub) (mockClientToServer, error) {
+	multiAddr, err := createNewMultiAddress()
+	if err != nil {
+		return mockClientToServer{}, err
+	}
+
+	return mockClientToServer{
+		multiAddr: multiAddr,
+		serverHub: mockServerHub,
+	}, nil
+}
+
 func (client *mockClientToServer) Ping(ctx context.Context, to identity.MultiAddress) (identity.MultiAddress, error) {
-	return client.serverHub.conns[to.Address()].Ping(ctx, to)
+	return client.serverHub.conns[to.Address()].Ping(ctx, client.multiAddr)
 }
 
 func (client *mockClientToServer) Query(ctx context.Context, to identity.MultiAddress, query identity.Address, querySig [65]byte) (identity.MultiAddresses, error) {
@@ -161,14 +125,26 @@ func (client *mockClientToServer) Query(ctx context.Context, to identity.MultiAd
 }
 
 func (client *mockClientToServer) MultiAddress() identity.MultiAddress {
-	return client.multiaddr
+	return client.multiAddr
 }
 
 type mockClient struct {
-	multiaddr   identity.MultiAddress
+	multiAddr   identity.MultiAddress
 	storedAddrs identity.MultiAddresses
 }
 
+func newMockClient(multiAddrs identity.MultiAddresses) (Client, error) {
+	multiAddr, err := createNewMultiAddress()
+	if err != nil {
+		return nil, err
+	}
+
+	return &mockClient{
+		multiAddr:   multiAddr,
+		storedAddrs: multiAddrs,
+	}, nil
+}
+
 func (client *mockClient) Ping(ctx context.Context, to identity.MultiAddress) (identity.MultiAddress, error) {
 	return to, nil
 }
@@ -178,42 +154,18 @@ func (client *mockClient) Query(ctx context.Context, to identity.MultiAddress, q
 }
 
 func (client *mockClient) MultiAddress() identity.MultiAddress {
-	return client.multiaddr
+	return client.multiAddr
 }
 
 func createNewMultiAddress() (identity.MultiAddress, error) {
-	// Generate multiaddress
+	// Generate multiAddress
 	ecdsaKey, err := crypto.RandomEcdsaKey()
 	if err != nil {
 		return identity.MultiAddress{}, err
 	}
-	multiaddr, err := identity.Address(ecdsaKey.Address()).MultiAddress()
+	multiAddr, err := identity.Address(ecdsaKey.Address()).MultiAddress()
 	if err != nil {
 		return identity.MultiAddress{}, err
 	}
-	return multiaddr, nil
-}
-
-func newMockClient(multiaddrs identity.MultiAddresses) (Client, error) {
-	multiaddr, err := createNewMultiAddress()
-	if err != nil {
-		return nil, err
-	}
-
-	return &mockClient{
-		multiaddr:   multiaddr,
-		storedAddrs: multiaddrs,
-	}, nil
-}
-
-func newMockClientToServer(mockServerHub *mockServerHub) (mockClientToServer, error) {
-	multiaddr, err := createNewMultiAddress()
-	if err != nil {
-		return mockClientToServer{}, err
-	}
-
-	return mockClientToServer{
-		multiaddr: multiaddr,
-		serverHub: mockServerHub,
-	}, nil
+	return multiAddr, nil
 }
